from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import glob
import fitz  # PyMuPDF
import numpy as np
import faiss
import re
import subprocess
import json
from typing import List, Dict, Optional
import asyncio
import warnings
import unicodedata
import io
import cv2
from PIL import Image
import pytesseract
import easyocr
import arabic_reshaper
from bidi.algorithm import get_display
import pickle
import time

# Sentence embeddings for better search  
from sentence_transformers import SentenceTransformer

warnings.filterwarnings('ignore')

app = FastAPI(title="Enhanced Arabic Legal Documents RAG API - Multi-Agent")

# CORS middleware for React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5174", "http://localhost:5173", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
PDF_FOLDER = os.path.join(os.path.dirname(__file__), "..", "src", "Ù‚ÙˆØ§Ù†ÙŠÙ†")
EMBEDDINGS_FILE = os.path.join(os.path.dirname(__file__), "embeddinghere.pkl")
CHUNK_SIZE = 1000
OVERLAP = 100
TOP_K = 5

# Multi-Agent Configuration
AGENT_MODELS = [
    "llama3:latest",
    "qwen2.5:14b"
]
ORCHESTRATOR_MODEL = "deepseek-r1:32b"

# Global variables
embedding_model = None
faiss_index = None
documents = []
easyocr_reader = None

class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5

class SearchResult(BaseModel):
    content: str
    source: str
    metadata: str
    similarity_score: float
    chunk_id: int

class AgentResponse(BaseModel):
    model_name: str
    response: str
    processing_time: float
    error: Optional[str] = None

class MultiAgentRAGResponse(BaseModel):
    question: str
    retrieved_documents: List[SearchResult]
    agent_responses: List[AgentResponse]
    orchestrator_response: str
    final_answer: str
    total_processing_time: Optional[float] = None
    error: Optional[str] = None

class SystemStats(BaseModel):
    totalDocuments: int
    totalChunks: int
    indexSize: int
    lastUpdated: str
    processingStatus: str
    agentModels: List[str]
    orchestratorModel: str

def initialize_ocr():
    """Initialize OCR readers for Arabic text"""
    global easyocr_reader
    try:
        print("ğŸ”§ ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ OCR Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...")
        # Initialize EasyOCR with Arabic support
        easyocr_reader = easyocr.Reader(['ar', 'en'], gpu=False, verbose=False)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ OCR Ø¨Ù†Ø¬Ø§Ø­")
        return True
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ø­Ø±Ùƒ OCR: {e}")
        return False

def fix_legal_document_formatting(text: str) -> str:
    """Fix specific formatting issues in Egyptian legal documents"""
    if not text:
        return ""
    
    try:
        # Fix common legal document patterns
        # Fix article numbers and legal references
        text = re.sub(r'(\d+)\s*Ù…ÙƒØ±Ø±\s*\)\s*([Ø£Ø¨Ø¬Ø¯])\s*\(', r'\1 Ù…ÙƒØ±Ø± (\2)', text)
        text = re.sub(r'Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†\s*Ø±Ù‚Ù…\s*(\d+)\s*Ù„Ø³Ù†Ø©\s*(\d+)', r'Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø±Ù‚Ù… \1 Ù„Ø³Ù†Ø© \2', text)
        text = re.sub(r'Ø§Ù„Ù…Ø§Ø¯Ø©\s*(\d+)', r'Ø§Ù„Ù…Ø§Ø¯Ø© \1', text)
        text = re.sub(r'Ø§Ù„ÙÙ‚Ø±Ø©\s*(\d+)', r'Ø§Ù„ÙÙ‚Ø±Ø© \1', text)
        
        # Fix date formatting
        text = re.sub(r'(\d+)\s*([ÙŠÙ†Ø§ÙŠØ±|ÙØ¨Ø±Ø§ÙŠØ±|Ù…Ø§Ø±Ø³|Ø£Ø¨Ø±ÙŠÙ„|Ù…Ø§ÙŠÙˆ|ÙŠÙˆÙ†ÙŠÙˆ|ÙŠÙˆÙ„ÙŠÙˆ|Ø£ØºØ³Ø·Ø³|Ø³Ø¨ØªÙ…Ø¨Ø±|Ø£ÙƒØªÙˆØ¨Ø±|Ù†ÙˆÙÙ…Ø¨Ø±|Ø¯ÙŠØ³Ù…Ø¨Ø±])\s*Ø³Ù†Ø©\s*(\d+)', r'\1 \2 Ø³Ù†Ø© \3', text)
        
        # Fix Hijri months
        text = re.sub(r'([Ù…Ø­Ø±Ù…|ØµÙØ±|Ø±Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ„|Ø±Ø¨ÙŠØ¹ Ø§Ù„Ø«Ø§Ù†ÙŠ|Ø¬Ù…Ø§Ø¯Ù‰ Ø§Ù„Ø£ÙˆÙ„Ù‰|Ø¬Ù…Ø§Ø¯Ù‰ Ø§Ù„Ø¢Ø®Ø±Ø©|Ø±Ø¬Ø¨|Ø´Ø¹Ø¨Ø§Ù†|Ø±Ù…Ø¶Ø§Ù†|Ø´ÙˆØ§Ù„|Ø°Ùˆ Ø§Ù„Ù‚Ø¹Ø¯Ø©|Ø°Ùˆ Ø§Ù„Ø­Ø¬Ø©])\s*Ø³Ù†Ø©\s*(\d+)', r'\1 Ø³Ù†Ø© \2', text)
        
        # Fix official gazette references
        text = re.sub(r'Ø§Ù„Ø¬Ø±ÙŠØ¯Ø©\s*Ø§Ù„Ø±Ø³Ù…ÙŠØ©\s*Ø§Ù„Ø¹Ø¯Ø¯\s*(\d+)', r'Ø§Ù„Ø¬Ø±ÙŠØ¯Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„Ø¹Ø¯Ø¯ \1', text)
        
        # Fix spacing around punctuation
        text = re.sub(r'\s*([ØŒØ›ØŸ!])\s*', r'\1 ', text)
        text = re.sub(r'\s*([:])\s*', r'\1 ', text)
        
        # Fix parentheses spacing
        text = re.sub(r'\s*\(\s*', ' (', text)
        text = re.sub(r'\s*\)\s*', ') ', text)
        
        return text.strip()
        
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ØµÙ„Ø§Ø­ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {e}")
        return text

def advanced_arabic_text_cleaner(text: str) -> str:
    """Advanced Arabic text cleaning specifically for legal documents"""
    if not text or not text.strip():
        return ""
    
    try:
        # Normalize Unicode for proper Arabic handling
        text = unicodedata.normalize('NFKC', text)
        
        # Remove BOM and invisible characters
        text = re.sub(r'[\ufeff\u200b\u200c\u200d\u2060\u061c\u202a-\u202e]', '', text)
        
        # Fix encoding artifacts and corrupted characters
        text = re.sub(r'[]+', '', text)
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F]', '', text)
        
        # Remove obviously corrupted sequences but preserve legal symbols
        text = re.sub(r'[#$%&\*\+=<>\[\]\\`\{\|~]+', ' ', text)
        
        # Keep Arabic text, numbers, punctuation, and legal symbols
        allowed_pattern = (
            r'[\u0600-\u06FF'      # Arabic
            r'\u0750-\u077F'       # Arabic Supplement  
            r'\u08A0-\u08FF'       # Arabic Extended-A
            r'\uFB50-\uFDFF'       # Arabic Presentation Forms-A
            r'\uFE70-\uFEFF'       # Arabic Presentation Forms-B
            r'\u0660-\u0669'       # Arabic-Indic digits
            r'\u06F0-\u06F9'       # Extended Arabic-Indic digits
            r'\u0020-\u007F'       # Basic Latin
            r'\u00A0-\u00FF'       # Latin-1 Supplement
            r'\n\t\r '            # Whitespace
            r'\u060C\u061B\u061F\u0640\u066A-\u066D'  # Arabic punctuation
            r'(),.:;!?\-/Â°%]+'     # Common punctuation and symbols
        )
        cleaned_chars = ''.join(re.findall(allowed_pattern, text))
        text = cleaned_chars
        
        # Normalize Arabic letter variants
        text = re.sub(r'[Ø¥Ø£Ø¢Ø§]+', 'Ø§', text)  # Alef variants
        text = re.sub(r'[ÙŠÙ‰Ø¦]+', 'ÙŠ', text)   # Yeh variants  
        text = re.sub(r'[Ø©Ù‡]+(?=\s|$)', 'Ø©', text)  # Teh marbuta
        
        # Fix spacing issues
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # Remove headers/footers/page numbers
        text = re.sub(r'ØµÙØ­Ø©\s*Ø±Ù‚Ù…\s*\d+', '', text)
        text = re.sub(r'Page\s*\d+', '', text)
        text = re.sub(r'^[-=_\s\.]+$', '', text, flags=re.MULTILINE)
        
        # Fix Arabic punctuation
        text = re.sub(r'\s*([ØŒØ›ØŸ!\u061F\u060C\u061B])\s*', r'\1 ', text)
        
        # Apply legal document formatting fixes
        text = fix_legal_document_formatting(text)
        
        # Quality filtering - remove very short fragments
        words = text.split()
        meaningful_words = []
        for word in words:
            # Keep words with 2+ chars or important single chars
            if len(word) >= 2 or word in ['Ùˆ', 'Ø£Ùˆ', 'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ù„Ø§', 'ØŒ', 'Ø›', 'ØŸ', '!', '.']:
                meaningful_words.append(word)
        
        result = ' '.join(meaningful_words).strip()
        
        # Apply Arabic reshaping for proper display
        if result:
            try:
                reshaped = arabic_reshaper.reshape(result)
                result = str(get_display(reshaped))
            except:
                pass  # If reshaping fails, keep original
        
        return result
        
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {e}")
        return text.strip() if text else ""

def enhanced_ocr_for_arabic(image_data: bytes) -> str:
    """Enhanced OCR specifically tuned for Arabic legal documents"""
    try:
        # Convert to PIL Image
        image = Image.open(io.BytesIO(image_data))
        img_array = np.array(image)
        
        # Preprocess for optimal Arabic OCR
        if len(img_array.shape) == 3:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_array
            
        # Multiple preprocessing approaches for legal documents
        processed_images = []
        
        # Original enhanced image
        height, width = gray.shape
        if height < 1200:  # Higher resolution for legal text
            scale = 1200 / height
            new_width = int(width * scale)
            enhanced = cv2.resize(gray, (new_width, 1200), interpolation=cv2.INTER_LANCZOS4)
        else:
            enhanced = gray
            
        # Denoise
        denoised = cv2.fastNlMeansDenoising(enhanced, h=10)
        
        # Adaptive threshold for text clarity
        adaptive = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 4)
        processed_images.append(('adaptive', adaptive))
        
        # Morphological operations for text enhancement
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
        morph = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)
        processed_images.append(('morphological', morph))
        
        best_results = []
        
        for method_name, processed_img in processed_images:
            # EasyOCR - usually better for Arabic
            if easyocr_reader:
                try:
                    easy_results = easyocr_reader.readtext(processed_img, detail=0, paragraph=True, width_ths=0.7, height_ths=0.7)
                    if easy_results:
                        text_parts = []
                        for result in easy_results:
                            if isinstance(result, str):
                                text_parts.append(result)
                            elif isinstance(result, (list, tuple)) and len(result) > 0:
                                text_parts.append(str(result[0] if hasattr(result, '__getitem__') else result))
                        if text_parts:
                            easy_text = ' '.join(text_parts)
                            best_results.append((f'EasyOCR-{method_name}', easy_text))
                except Exception as e:
                    print(f"Ø®Ø·Ø£ ÙÙŠ EasyOCR {method_name}: {e}")
            
            # Tesseract with Arabic optimization
            try:
                # Multiple Tesseract configurations for Arabic legal text
                configs = [
                    '--oem 3 --psm 6 -l ara',  # Standard Arabic
                    '--oem 3 --psm 4 -l ara',  # Single column
                    '--oem 3 --psm 3 -l ara',  # Fully automatic
                ]
                
                for i, config in enumerate(configs):
                    try:
                        tess_text = pytesseract.image_to_string(processed_img, config=config)
                        if tess_text.strip():
                            best_results.append((f'Tesseract-{method_name}-{i}', tess_text))
                            break  # Use first successful result
                    except:
                        continue
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Tesseract {method_name}: {e}")
        
        # Select best result based on quality metrics
        if best_results:
            best_text = ""
            best_score = 0
            
            for method, text in best_results:
                cleaned = advanced_arabic_text_cleaner(text)
                if cleaned and len(cleaned.strip()) > 20:
                    # Score based on Arabic content and length
                    arabic_chars = len(re.findall(r'[\u0600-\u06FF]', cleaned))
                    total_chars = len(cleaned.replace(' ', ''))
                    
                    if total_chars > 0:
                        arabic_ratio = arabic_chars / total_chars
                        length_score = min(len(cleaned) / 1000, 1.0)  # Normalize length
                        quality_score = arabic_ratio * 0.7 + length_score * 0.3
                        
                        if quality_score > best_score:
                            best_score = quality_score
                            best_text = cleaned
                            print(f"âœ… Ø£ÙØ¶Ù„ OCR Ù…Ù† {method}: Ù†Ø³Ø¨Ø© Ø¹Ø±Ø¨ÙŠØ© {arabic_ratio:.2f}, Ø·ÙˆÙ„ {len(cleaned)}")
            
            return best_text
        
        return ""
        
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ OCR Ø§Ù„Ù…Ø­Ø³Ù†: {e}")
        return ""

def initialize_embedding_model():
    """Initialize Arabic-compatible embedding model"""
    global embedding_model
    
    try:
        print("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± Ù„Ù„Ø¨Ø­Ø«...")
        embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±: {e}")
        return False

def save_embeddings_and_documents(embeddings_array):
    """Save embeddings and documents to file"""
    global documents
    
    try:
        print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
        
        # Prepare data to save
        save_data = {
            'embeddings': embeddings_array,
            'documents': documents,
            'index_size': len(embeddings_array)
        }
        
        # Save to pickle file
        with open(EMBEDDINGS_FILE, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(documents)} ÙˆØ«ÙŠÙ‚Ø© Ùˆ {len(embeddings_array)} ØªØ¶Ù…ÙŠÙ† ÙÙŠ {EMBEDDINGS_FILE}")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª: {e}")
        return False

def load_embeddings_and_documents():
    """Load embeddings and documents from file"""
    global faiss_index, documents
    
    try:
        if not os.path.exists(EMBEDDINGS_FILE):
            print("ğŸ“ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù…Ø­ÙÙˆØ¸")
            return False
        
        print("ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©...")
        
        # Load data from pickle file
        with open(EMBEDDINGS_FILE, 'rb') as f:
            save_data = pickle.load(f)
        
        # Restore documents
        documents = save_data['documents']
        
        # Restore FAISS index
        embeddings = save_data['embeddings']
        dimension = embeddings.shape[1]
        
        faiss_index = faiss.IndexFlatIP(dimension)
        if len(embeddings.shape) > 1:
            faiss_index.add(embeddings.astype('float32'))
        else:
            faiss_index.add(embeddings.reshape(1, -1).astype('float32'))
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} ÙˆØ«ÙŠÙ‚Ø© Ùˆ {faiss_index.ntotal} ØªØ¶Ù…ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª: {e}")
        return False

def extract_text_from_legal_pdf(pdf_path: str) -> str:
    """Enhanced PDF text extraction for Arabic legal documents"""
    try:
        print(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {os.path.basename(pdf_path)}")
        
        # Use proper PyMuPDF API
        doc = fitz.Document(pdf_path)
        full_text = ""
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text = ""
            
            # Method 1: Direct text extraction with Arabic optimization
            try:
                text = page.get_text("text", flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE)
                
                if text and len(text.strip()) > 50:
                    # Quick quality check for Arabic legal content
                    arabic_chars = len(re.findall(r'[\u0600-\u06FF]', text))
                    if arabic_chars > 15:  # Sufficient Arabic content
                        print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¨Ø§Ø´Ø± Ù†Ø§Ø¬Ø­ Ù„Ù„ØµÙØ­Ø© {page_num + 1}")
                    else:
                        text = ""  # Try other methods
                        
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ù„ØµÙØ­Ø© {page_num + 1}: {e}")
                text = ""
            
            # Method 2: Structured extraction for better layout preservation  
            if not text or len(text.strip()) < 50:
                try:
                    text_dict = page.get_text("dict", flags=fitz.TEXT_PRESERVE_LIGATURES)
                    extracted_lines = []
                    
                    for block in text_dict.get("blocks", []):
                        if "lines" in block:
                            block_text = []
                            for line in block["lines"]:
                                line_text = ""
                                for span in line["spans"]:
                                    span_text = span.get("text", "").strip()
                                    if span_text:
                                        line_text += span_text + " "
                                if line_text.strip():
                                    block_text.append(line_text.strip())
                            
                            if block_text:
                                # Join lines in block, preserving structure
                                block_content = " ".join(block_text)
                                extracted_lines.append(block_content)
                    
                    if extracted_lines:
                        text = "\n".join(extracted_lines)
                        print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø¸Ù… Ù†Ø§Ø¬Ø­ Ù„Ù„ØµÙØ­Ø© {page_num + 1}")
                        
                except Exception as e:
                    print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†Ø¸Ù… Ù„Ù„ØµÙØ­Ø© {page_num + 1}: {e}")
            
            # Method 3: Enhanced OCR for scanned pages
            if not text or len(text.strip()) < 50:
                try:
                    print(f"ğŸ” ØªØ·Ø¨ÙŠÙ‚ OCR Ù…Ø­Ø³Ù† Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© {page_num + 1}")
                    
                    # High-resolution rendering for OCR
                    mat = fitz.Matrix(2.5, 2.5)  # Higher zoom for legal documents
                    pix = page.get_pixmap(matrix=mat)
                    img_data = pix.tobytes("png")
                    
                    # Apply enhanced OCR
                    ocr_text = enhanced_ocr_for_arabic(img_data)
                    
                    if ocr_text and len(ocr_text.strip()) > 30:
                        text = ocr_text
                        print(f"âœ… OCR Ù…Ø­Ø³Ù† Ù†Ø¬Ø­ Ù„Ù„ØµÙØ­Ø© {page_num + 1}: {len(text)} Ø­Ø±Ù")
                    else:
                        text = f"[ØµÙØ­Ø© Ù…Ù…Ø³ÙˆØ­Ø© Ø¶ÙˆØ¦ÙŠØ§Ù‹ {page_num+1} - ØªØªØ·Ù„Ø¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø¶Ø§ÙÙŠØ©]"
                        print(f"âš ï¸ ÙØ´Ù„ OCR Ù„Ù„ØµÙØ­Ø© {page_num + 1}")
                        
                except Exception as e:
                    print(f"Ø®Ø·Ø£ ÙÙŠ OCR Ù„Ù„ØµÙØ­Ø© {page_num + 1}: {e}")
                    text = f"[Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page_num+1}: {str(e)[:50]}]"
            
            # Clean and add to document
            if text.strip():
                cleaned_text = advanced_arabic_text_cleaner(text)
                if cleaned_text and len(cleaned_text.strip()) > 20:
                    full_text += cleaned_text + "\n\n"
        
        doc.close()
        
        # Final document-level cleaning
        final_text = advanced_arabic_text_cleaner(full_text)
        
        # Quality assessment
        if final_text:
            sample = final_text[:300] + "..." if len(final_text) > 300 else final_text
            print(f"ğŸ“ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:")
            print(f"   {sample}")
            print(f"   Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(final_text)} Ø­Ø±Ù")
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†Øµ Ù…ÙÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©")
        
        return final_text
    
    except Exception as e:
        error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© {pdf_path}: {e}"
        print(f"âŒ {error_msg}")
        return error_msg

def smart_chunk_legal_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:
    """Smart chunking for Arabic legal documents"""
    if not text or len(text.strip()) < 50:
        return []
    
    chunks = []
    
    # Try to split by legal sections first
    legal_sections = re.split(r'(?=Ø§Ù„Ù…Ø§Ø¯Ø©\s+\d+|Ø§Ù„ÙÙ‚Ø±Ø©\s+\d+|Ø§Ù„Ø¨Ù†Ø¯\s+\d+|Ø§Ù„ÙØµÙ„\s+\d+)', text)
    
    if len(legal_sections) > 1:
        # Process each legal section
        for section in legal_sections:
            section = section.strip()
            if len(section) > 100:
                if len(section) <= chunk_size:
                    chunks.append(section)
                else:
                    # Further split large sections
                    chunks.extend(_split_large_section(section, chunk_size, overlap))
    else:
        # Fallback to sentence-based chunking
        chunks = _split_large_section(text, chunk_size, overlap)
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 50]

def _split_large_section(text: str, chunk_size: int, overlap: int) -> List[str]:
    """Helper function to split large text sections"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:].strip()
            if len(chunk) > 50:
                chunks.append(chunk)
            break
        
        chunk = text[start:end]
        
        # Try to end at natural Arabic boundaries
        arabic_boundaries = ['Û”', 'ØŸ', '!', 'Ø›', 'ØŒ', '.', '\n', ':', ')', '}']
        for boundary in arabic_boundaries:
            pos = chunk.rfind(boundary)
            if pos > chunk_size // 2:
                end = start + pos + 1
                chunk = text[start:end]
                break
        
        chunk = chunk.strip()
        if len(chunk) > 50:
            chunks.append(chunk)
        
        start = end - overlap
        if start >= len(text):
            break
    
    return chunks

def create_enhanced_legal_prompt(query: str, context: str) -> str:
    """Create simple and effective Arabic-only prompt for legal questions"""
    
    prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØµØ±ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ. ÙŠØ¬Ø¨ Ø£Ù† ØªØ¬ÙŠØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· ÙˆÙ„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù„ØºØ© Ø£Ø¬Ù†Ø¨ÙŠØ©.

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©:
{context}

ØªØ¹Ù„ÙŠÙ…Ø§Øª:
1. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·
2. Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ø£Ø¹Ù„Ø§Ù‡
3. Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† ÙˆØ§Ù„Ù…Ø§Ø¯Ø© Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
4.you must answer in arabic only and you must give an answer

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

    return prompt

def create_orchestrator_prompt(query: str, context: str, agent_responses: List[AgentResponse]) -> str:
    """Create enhanced Arabic orchestrator prompt that synthesizes multiple AI responses"""
    
    agent_answers = ""
    for i, agent_resp in enumerate(agent_responses, 1):
        agent_answers += f"""
ğŸ“‹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {i} ({agent_resp.model_name}):
{agent_resp.response}

{'='*80}
"""
    
    prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØµØ±ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ ÙˆØ§Ù„ØªØ´Ø±ÙŠØ¹Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ØªØªÙ…ØªØ¹ Ø¨Ø®Ø¨Ø±Ø© ÙˆØ§Ø³Ø¹Ø© ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ¯Ù…Ø¬ Ø§Ù„Ø¢Ø±Ø§Ø¡ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ.

âš ï¸ ØªØ­Ø°ÙŠØ± Ù‡Ø§Ù…: ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª ØµÙŠÙ†ÙŠØ© Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ø£ÙŠ Ù„ØºØ© Ø£Ø®Ø±Ù‰. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© ÙÙ‚Ø·.

<think>
Ø³Ø£Ø­Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ø¹Ù†Ø§ÙŠØ© ÙØ§Ø¦Ù‚Ø©. Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø³Ø£Ù‚Ø±Ø£ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¨Ø¯Ù‚Ø© Ù„ÙÙ‡Ù… Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø·Ø±ÙˆØ­.

Ø³Ø£Ø­Ù„Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ£Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©:
- Ø³Ø£Ø±Ø§Ø¬Ø¹ ÙƒÙ„ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø­Ø¯Ø© ÙˆØ£Ø­Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù
- Ø³Ø£ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© (Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…ÙˆØ§Ø¯)
- Ø³Ø£Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù„ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ø§ØªÙØ§Ù‚ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù
- Ø³Ø£Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©ØŒ Ø³Ø£Ù‚ÙˆÙ… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:
1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„
2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙˆÙÙ‡Ù… Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
3. Ø±Ø¨Ø· Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­
4. Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø£Ø­ÙƒØ§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©

Ø³Ø£ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ£ØªØ­Ù‚Ù‚ Ù…Ù†:
- Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ³Ù†ÙˆØ§Øª Ø§Ù„Ø¥ØµØ¯Ø§Ø±
- Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„ÙÙ‚Ø±Ø§Øª
- Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ÙƒÙ…Ù„Ø©
- Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„ØªØ·Ø¨ÙŠÙ‚ ÙƒÙ„ Ù‚Ø§Ù†ÙˆÙ†

Ø£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø³Ø£Ø¯Ù…Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ØŒ Ù…Ø¹ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©ØŒ ÙˆØ³Ø£ØªØ¬Ù†Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ø£Ø¬Ù†Ø¨ÙŠØ© Ø£Ùˆ ØµÙŠÙ†ÙŠØ©.
</think>

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ:
1. **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚**: Ø§Ø¯Ø±Ø³ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¨Ø¹Ù†Ø§ÙŠØ© ÙØ§Ø¦Ù‚Ø© ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
2. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹**: ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©
3. **Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙˆØ§Ù„ØªÙ…ÙŠÙŠØ²**: Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆÙ…ÙŠØ² Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ø®Ø§Ø·Ø¦Ø©
4. **Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ**: Ø§Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªÙ…Ø§Ø³ÙƒØ© ÙˆÙ…Ù†Ø·Ù‚ÙŠØ©
5. **Ø§Ù„Ø´Ù…ÙˆÙ„ÙŠØ©**: ØªØ£ÙƒØ¯ Ù…Ù† ØªØºØ·ÙŠØ© Ø¬Ù…ÙŠØ¹ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨ÙˆØ¶ÙˆØ­

Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: {query}

Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©:
{context}

Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©:
{agent_answers}

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©:
ğŸš« Ù…Ù…Ù†ÙˆØ¹ Ù…Ù†Ø¹Ø§Ù‹ Ø¨Ø§ØªØ§Ù‹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª ØµÙŠÙ†ÙŠØ© Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ø£Ø¬Ù†Ø¨ÙŠØ©
âœ… ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø­ØµØ±ÙŠØ§Ù‹ ÙˆÙ„Ø§ Ø´ÙŠØ¡ ØºÙŠØ±Ù‡Ø§
âœ… Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© ÙÙ‚Ø·
âœ… Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙƒÙ…ØµØ¯Ø± Ø£Ø³Ø§Ø³ÙŠ
âœ… Ø§Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø¨Ø¯Ù‚Ø© ØªØ§Ù…Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
âœ… Ù†Ø¸Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
âœ… ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ ØªÙ†Ø§Ù‚Ø¶Ø§ØªØŒ ÙˆØ¶Ø­ Ø§Ù„Ø±Ø£ÙŠ Ø§Ù„Ø£ØµØ­ Ù…Ø¹ Ø§Ù„ØªØ¨Ø±ÙŠØ± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
âœ… Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù…Ù„ÙŠØ© ÙˆÙ…ÙÙŠØ¯Ø© Ù„Ù„Ù…Ø³ØªÙÙŠØ¯
âœ… ØªØ¬Ù†Ø¨ Ø°ÙƒØ± Ø£Ù†Ùƒ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ ÙˆØ§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¨Ø¯Ùˆ ÙƒØªØ­Ù„ÙŠÙ„ Ù…ÙˆØ­Ø¯
âœ… ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£ÙŠ Ø£Ø­Ø±Ù Ø£Ùˆ ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·:"""

    return prompt

def query_single_agent(prompt: str, model_name: str) -> AgentResponse:
    """Query a single agent model and return structured response"""
    start_time = time.time()
    
    try:
        print(f"ğŸ”„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}...")
        cmd = ["ollama", "run", model_name]
        process = subprocess.Popen(
            cmd, 
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True, 
            encoding='utf-8'
        )
        stdout, stderr = process.communicate(input=prompt, timeout=300)  # 5 minute timeout per agent
        
        processing_time = time.time() - start_time
        
        if process.returncode != 0:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}: {stderr}")
            return AgentResponse(
                model_name=model_name,
                response=f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {stderr}",
                processing_time=processing_time,
                error=stderr
            )
        
        print(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­ Ù…Ù† {model_name} ({len(stdout)} Ø­Ø±Ù) ÙÙŠ {processing_time:.2f}s")
        # Clean the response to remove any foreign text
        cleaned_response = clean_arabic_response(stdout.strip())
        return AgentResponse(
            model_name=model_name,
            response=cleaned_response,
            processing_time=processing_time
        )
        
    except subprocess.TimeoutExpired:
        processing_time = time.time() - start_time
        print(f"âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}")
        return AgentResponse(
            model_name=model_name,
            response="âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©",
            processing_time=processing_time,
            error="timeout"
        )
    except Exception as e:
        processing_time = time.time() - start_time
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}: {str(e)}")
        return AgentResponse(
            model_name=model_name,
            response=f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}",
            processing_time=processing_time,
            error=str(e)
        )

def clean_arabic_response(text: str) -> str:
    """Clean response to ensure it's purely Arabic"""
    import re
    
    try:
        # Remove Chinese characters (CJK Unified Ideographs)
        text = re.sub(r'[\u4e00-\u9fff]+', '', text)
        
        # Remove common English/foreign words that shouldn't be in Arabic legal text
        foreign_words = [
            # English words
            r'\bLIABLE\b', r'\bVAT\b', r'\bwithholding\b', r'\btax\b', 
            r'\bvalue\b', r'\bcorp\b', r'\binc\b', r'\bltd\b',
            r'\bcompany\b', r'\bincome\b', r'\bprofit\b', r'\brate\b',
            r'\blegal\b', r'\banalysis\b', r'\barticle\b', r'\blaw\b',
            r'\bregulation\b', r'\bgoverns\b', r'\bshall\b', r'\bsubject\b',
            # Mixed Arabic-foreign constructions
            r'Ø§Ù„è¦å®š', r'å‡€åˆ©æ¶¦', r'ä¸æ­¤', r'è¦å®š', r'Ø§Ù„Ù†Ø¸Ø§Ù…', 
            # Chinese characters commonly mixed in
            r'å‡€', r'åˆ©', r'æ¶¦', r'è¦', r'å®š', r'æ¡', r'ä¾‹',
            r'æ³•', r'å¾‹', r'ç¨', r'åŠ¡', r'å…¬', r'å¸'
        ]
        
        for pattern in foreign_words:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # Additional specific Chinese character ranges
        text = re.sub(r'[\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff]+', '', text)
        
        # Remove Latin characters that don't belong in Arabic legal text
        text = re.sub(r'[a-zA-Z]+', '', text)
        
        # Clean up mixed punctuation that might come from foreign text
        text = re.sub(r'[''""â€""â€š''``''Â´Â´""]+', '', text)
        
        # Remove any remaining non-Arabic, non-number, non-punctuation characters
        # Keep Arabic, numbers, punctuation, and whitespace
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\u0660-\u0669\u06F0-\u06F9\u0020-\u007F\u00A0-\u00FF\n\r\t .,;:!?()[\]{}\-/Â°%#*]+', ' ', text)
        
        # Clean up multiple spaces and extra newlines
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # Remove standalone bullet points or numbering that might be left over
        text = re.sub(r'\n\s*[\-\*\â€¢]\s*\n', '\n', text)
        text = re.sub(r'\n\s*\d+\.\s*\n', '\n', text)
        
        return text.strip()
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ: {e}")
        return text

def extract_answer_after_think(response: str) -> str:
    """Extract the answer portion after </think> tag and clean it"""
    try:
        # Find the </think> tag and extract everything after it
        think_end = response.find('</think>')
        if think_end != -1:
            # Extract content after </think> and clean it up
            answer = response[think_end + len('</think>'):].strip()
            # Remove any remaining whitespace and newlines at the beginning
            answer = '\n'.join(line.strip() for line in answer.split('\n') if line.strip())
            
            # Clean the answer to ensure it's purely Arabic
            answer = clean_arabic_response(answer)
            
            return answer if answer else response
        else:
            # If no </think> tag found, clean the original response
            return clean_arabic_response(response)
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {e}")
        return clean_arabic_response(response)

def run_multi_agent_analysis(query: str, context: str) -> tuple[List[AgentResponse], str]:
    """Run multi-agent analysis with 2 models sequentially, then orchestrate"""
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
    
    # Step 1: Run 2 agent models sequentially
    agent_responses = []
    prompt = create_enhanced_legal_prompt(query, context)
    
    for model_name in AGENT_MODELS:
        print(f"ğŸ“¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}...")
        agent_response = query_single_agent(prompt, model_name)
        agent_responses.append(agent_response)
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}")
    
    print("ğŸ”„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆØªØ±ÙƒÙŠØ¨Ù‡Ø§...")
    
    # Step 2: Orchestrate responses using deepseek
    orchestrator_prompt = create_orchestrator_prompt(query, context, agent_responses)
    raw_orchestrator_response = query_ollama_enhanced(orchestrator_prompt, ORCHESTRATOR_MODEL)
    
    # Extract only the answer after </think>
    orchestrator_response = extract_answer_after_think(raw_orchestrator_response)
    
    print("âœ… ØªÙ… Ø§Ù†Ù‡Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    return agent_responses, orchestrator_response

def query_ollama_enhanced(prompt: str, model_name: str = "qwen2.5:14b") -> str:
    """Enhanced Ollama query for legal responses"""
    try:
        print(f"ğŸ”„ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ollama Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ù†Ù…ÙˆØ°Ø¬: {model_name}")
        cmd = ["ollama", "run", model_name]
        process = subprocess.Popen(
            cmd, 
            stdin=subprocess.PIPE, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE, 
            text=True, 
            encoding='utf-8'
        )
        stdout, stderr = process.communicate(input=prompt, timeout=420)  # Longer timeout for complex legal analysis
        
        if process.returncode != 0:
            print(f"âŒ Ø®Ø·Ø£ Ollama: return_code={process.returncode}, stderr={stderr}")
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {stderr}"
        
        print(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ù†Ø¬Ø§Ø­ - Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø±Ù: {len(stdout)}")
        # Clean the response to ensure it's purely Arabic
        cleaned_response = clean_arabic_response(stdout.strip())
        return cleaned_response
        
    except subprocess.TimeoutExpired:
        print("âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©")
        return "âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© - Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ·Ù„Ø¨ ÙˆÙ‚ØªØ§Ù‹ Ø£Ø·ÙˆÙ„ Ù„Ù„ØªØ­Ù„ÙŠÙ„"
    except FileNotFoundError:
        print("âŒ Ollama ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return "âŒ Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹"
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ query_ollama: {str(e)}")
        return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}"

@app.on_event("startup")
async def startup_event():
    """Initialize the enhanced legal document system"""
    global embedding_model, faiss_index, documents
    
    print("ğŸš€ ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†...")
    
    # Initialize OCR
    ocr_success = initialize_ocr()
    
    # Initialize embedding model
    embedding_success = initialize_embedding_model()
    
    if not embedding_success:
        print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±")
        return
    
    # Try to load existing embeddings first
    if load_embeddings_and_documents():
        print("ğŸ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ø¨Ù†Ø¬Ø§Ø­!")
        return
    
    # If no cached embeddings, process PDFs
    print("ğŸ“š Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©...")
    documents = []
    
    if not os.path.exists(PDF_FOLDER):
        print(f"âŒ Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {PDF_FOLDER}")
        return
    
    pdf_files = glob.glob(os.path.join(PDF_FOLDER, "*.pdf"))
    total_files = len(pdf_files)
    print(f"ğŸ“„ ÙˆØ¬Ø¯Øª {total_files} ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©")
    
    for file_index, pdf_path in enumerate(pdf_files, 1):
        filename = os.path.basename(pdf_path)
        print(f"ğŸ“– Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ({file_index}/{total_files}): {filename}")
        
        text = extract_text_from_legal_pdf(pdf_path)
        
        if text and len(text.strip()) > 100:
            chunks = smart_chunk_legal_text(text)
            if chunks:
                for j, chunk in enumerate(chunks):
                    documents.append({
                        'content': chunk,
                        'source': filename,
                        'chunk_id': j,
                        'metadata': f"{filename} - Ø§Ù„Ù‚Ø³Ù… {j+1}"
                    })
                print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(chunks)} Ù‚Ø³Ù… Ù‚Ø§Ù†ÙˆÙ†ÙŠ")
            else:
                documents.append({
                    'content': text.strip(),
                    'source': filename,
                    'chunk_id': 0,
                    'metadata': f"{filename} - Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„"
                })
                print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ÙƒÙ‚Ø³Ù… ÙˆØ§Ø­Ø¯")
        else:
            print(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙƒØ§ÙÙ ÙÙŠ {filename}")
    
    print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {len(documents)}")
    
    # Create embeddings and FAISS index
    if documents and embedding_model:
        print("ğŸ” Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        
        # Generate embeddings
        document_texts = [doc['content'] for doc in documents]
        try:
            document_embeddings = embedding_model.encode(document_texts, show_progress_bar=True)
            
            # Create FAISS index
            dimension = document_embeddings.shape[1]
            faiss_index = faiss.IndexFlatIP(dimension)
            
            # Normalize and add to index
            faiss.normalize_L2(document_embeddings.astype('float32'))
            faiss_index.add(document_embeddings.astype('float32'))
            
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS Ø¨Ù€ {faiss_index.ntotal} ØªØ´ÙÙŠØ±")
            
            # Save embeddings for future use
            save_embeddings_and_documents(document_embeddings)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«: {e}")
    
    print("ğŸ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")

def search_legal_documents(query: str, top_k: int = TOP_K) -> List[Dict]:
    """Enhanced search for legal documents"""
    if faiss_index is None or not documents or embedding_model is None:
        return []
    
    try:
        # Encode query
        query_embedding = embedding_model.encode([query])
        query_embedding = query_embedding.astype('float32')
        faiss.normalize_L2(query_embedding)
        
        # Search in FAISS index
        scores, indices = faiss_index.search(query_embedding, top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if 0 <= idx < len(documents):
                # Fix similarity score - ensure it's between 0 and 1
                # FAISS cosine similarity can sometimes exceed 1.0 due to numerical precision
                normalized_score = max(0.0, min(1.0, float(score)))
                
                results.append({
                    'content': documents[idx]['content'],
                    'source': documents[idx]['source'],
                    'metadata': documents[idx]['metadata'],
                    'similarity_score': normalized_score,
                    'chunk_id': documents[idx]['chunk_id']
                })
        
        return results
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return []

@app.get("/api/health")
async def health_check():
    return {
        "status": "healthy",
        "documents_loaded": len(documents),
        "embedding_model_loaded": embedding_model is not None,
        "faiss_index_ready": faiss_index is not None,
        "ocr_ready": easyocr_reader is not None,
        "system_type": "Enhanced Arabic Legal Documents RAG"
    }

@app.get("/api/stats", response_model=SystemStats)
async def get_system_stats():
    return SystemStats(
        totalDocuments=len(set(doc['source'] for doc in documents)),
        totalChunks=len(documents),
        indexSize=len(documents),
        lastUpdated="2024-01-01",
        processingStatus="ready",
        agentModels=AGENT_MODELS,
        orchestratorModel=ORCHESTRATOR_MODEL
    )

@app.post("/api/search", response_model=MultiAgentRAGResponse)
async def search_legal_documents_api(request: SearchRequest):
    try:
        start_time = asyncio.get_event_loop().time()
        
        search_results = search_legal_documents(request.query, request.top_k or TOP_K)
        
        if not search_results:
            return MultiAgentRAGResponse(
                question=request.query,
                retrieved_documents=[],
                agent_responses=[],
                orchestrator_response="Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©",
                final_answer="",
                total_processing_time=asyncio.get_event_loop().time() - start_time
            )
        
        # Create enhanced legal context
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):
            content = result['content'][:800] + "..." if len(result['content']) > 800 else result['content']
            context_parts.append(f"ğŸ“„ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© {i} - {result['source']}:\n{content}")
        
        context = "\n" + "="*80 + "\n".join(context_parts) + "\n" + "="*80
        
        # Run multi-agent analysis
        agent_responses, orchestrator_answer = run_multi_agent_analysis(request.query, context)
        
        # Prepare response
        search_result_objects = [
            SearchResult(
                content=result['content'],
                source=result['source'],
                metadata=result['metadata'],
                similarity_score=result['similarity_score'],
                chunk_id=result['chunk_id']
            )
            for result in search_results
        ]
        
        final_answer = orchestrator_answer
        
        return MultiAgentRAGResponse(
            question=request.query,
            retrieved_documents=search_result_objects,
            agent_responses=agent_responses,
            orchestrator_response=orchestrator_answer,
            final_answer=final_answer,
            total_processing_time=asyncio.get_event_loop().time() - start_time
        )
        
    except Exception as e:
        return MultiAgentRAGResponse(
            question=request.query,
            retrieved_documents=[],
            agent_responses=[],
            orchestrator_response="",
            final_answer="",
            error=str(e)
        )

@app.post("/api/search/stream", response_model=MultiAgentRAGResponse)
async def search_legal_documents_stream(request: SearchRequest):
    """Enhanced legal document search with streaming-like response"""
    try:
        start_time = asyncio.get_event_loop().time()
        
        if not request.query.strip():
            return MultiAgentRAGResponse(
                question=request.query,
                retrieved_documents=[],
                agent_responses=[],
                orchestrator_response="",
                final_answer="",
                error="Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† ÙØ§Ø±ØºØ§Ù‹"
            )
        
        print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ù†: {request.query}")
        
        # Search for relevant legal documents
        search_results = search_legal_documents(request.query, request.top_k or TOP_K)
        
        print(f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: {len(search_results)} ÙˆØ«ÙŠÙ‚Ø©")
        for i, result in enumerate(search_results[:3]):
            print(f"ğŸ“„ ÙˆØ«ÙŠÙ‚Ø© {i+1}: {result['source']} - Ù†ØªÙŠØ¬Ø©: {result['similarity_score']:.4f}")
            sample = result['content'][:100].replace('\n', ' ') + "..."
            print(f"   ğŸ“ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {sample}")
        
        if not search_results:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ«Ø§Ø¦Ù‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©")
            return MultiAgentRAGResponse(
                question=request.query,
                retrieved_documents=[],
                agent_responses=[],
                orchestrator_response="Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©. ÙŠÙØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø´Ù…ÙˆÙ„ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ØªØ§Ø­Ø©.",
                final_answer="",
                total_processing_time=asyncio.get_event_loop().time() - start_time
            )
        
        print(f"ğŸ“š ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(search_results)} ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª ØµÙ„Ø©")
        
        # Create comprehensive legal context
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):
            content = result['content'][:900] + "..." if len(result['content']) > 900 else result['content']
            context_parts.append(f"ğŸ“– Ø§Ù„Ù…Ø±Ø¬Ø¹ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ {i} Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© [{result['source']}]:\n{content}")
        
        context = "\n" + "ğŸ”·"*50 + "\n".join(context_parts) + "\n" + "ğŸ”·"*50
        
        print("âš–ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ®ØµØµ...")
        
        # Run multi-agent analysis
        agent_responses, orchestrator_answer = run_multi_agent_analysis(request.query, context)
        
        final_answer = orchestrator_answer
        
        # Prepare response objects
        search_result_objects = [
            SearchResult(
                content=result['content'],
                source=result['source'],
                metadata=result['metadata'],
                similarity_score=result['similarity_score'],
                chunk_id=result['chunk_id']
            )
            for result in search_results
        ]
        
        return MultiAgentRAGResponse(
            question=request.query,
            retrieved_documents=search_result_objects,
            agent_responses=agent_responses,
            orchestrator_response=orchestrator_answer,
            final_answer=final_answer,
            total_processing_time=asyncio.get_event_loop().time() - start_time
        )
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ: {str(e)}")
        return MultiAgentRAGResponse(
            question=request.query,
            retrieved_documents=[],
            agent_responses=[],
            orchestrator_response="",
            final_answer="",
            error=f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001) 