from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import glob
import fitz  # PyMuPDF
import numpy as np
import faiss
import re
import json
from typing import List, Dict, Optional
import asyncio
import warnings
import unicodedata
import io
import cv2
from PIL import Image
import pytesseract
import arabic_reshaper
from bidi.algorithm import get_display
import pickle
import time
from groq import Groq
from langdetect import detect
from deep_translator import GoogleTranslator
from textblob import TextBlob
from spellchecker import SpellChecker

# Sentence embeddings for better search  
from sentence_transformers import SentenceTransformer

warnings.filterwarnings('ignore')

app = FastAPI(title="Arabic Legal RAG System with Agentic Workflow")

# CORS middleware for React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5174", "http://localhost:5173", "http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
PDF_FOLDER = os.path.join(os.path.dirname(__file__), "..", "src", "Ù‚ÙˆØ§Ù†ÙŠÙ†")
EMBEDDINGS_FILE = os.path.join(os.path.dirname(__file__), "embeddinghere.pkl")
CHUNK_SIZE = 1000
OVERLAP = 100
TOP_K = 5

# Groq Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "your-groq-api-key-here")
GROQ_MODEL = "meta-llama/llama-3.1-70b-versatile"

# Global variables
embedding_model = None
faiss_index = None
documents = []
groq_client = None
spell_checker = None

class ChatMessage(BaseModel):
    message: str

class AgentResponse(BaseModel):
    agent_name: str
    response: str
    processing_time: float
    error: Optional[str] = None

class SearchItem(BaseModel):
    query: str
    priority: int
    category: str

class RAGResult(BaseModel):
    search_item: str
    documents: List[Dict]
    summary: str

class AgentWorkflowResponse(BaseModel):
    collected_info: Dict[str, str]
    search_items: List[SearchItem]
    rag_results: List[RAGResult]
    final_recommendation: str
    processing_time: float
    error: Optional[str] = None

def initialize_groq():
    """Initialize Groq client"""
    global groq_client
    try:
        groq_client = Groq(api_key=GROQ_API_KEY)
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Groq Ø¨Ù†Ø¬Ø§Ø­")
        return True
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Groq: {e}")
        return False

def initialize_spell_checker():
    """Initialize spell checker for text correction"""
    global spell_checker
    try:
        spell_checker = SpellChecker(language='en')
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…ØµØ­Ø­ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­")
        return True
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…ØµØ­Ø­ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¡: {e}")
        return False

def correct_text(text: str) -> str:
    """Auto-correct text using spell checker"""
    if not spell_checker:
        return text
    
    try:
        # Split text into words
        words = text.split()
        corrected_words = []
        
        for word in words:
            # Clean word from punctuation for checking
            clean_word = re.sub(r'[^\w]', '', word.lower())
            if clean_word and clean_word in spell_checker:
                corrected_words.append(word)
            elif clean_word:
                # Get correction
                correction = spell_checker.correction(clean_word)
                if correction and correction != clean_word:
                    # Replace the clean part while keeping punctuation
                    corrected_word = word.replace(clean_word, correction)
                    corrected_words.append(corrected_word)
                else:
                    corrected_words.append(word)
            else:
                corrected_words.append(word)
        
        return ' '.join(corrected_words)
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Øµ: {e}")
        return text

def detect_and_translate_to_arabic(text: str) -> str:
    """Detect language and translate to Arabic if needed"""
    try:
        # Detect language
        detected_lang = detect(text)
        print(f"ğŸ” Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {detected_lang}")
        
        # If already Arabic, return as is
        if detected_lang == 'ar':
            return text
        
        # Translate to Arabic
        translator = GoogleTranslator(source=detected_lang, target='ar')
        
        # Split text into chunks for translation (Google Translator has limits)
        max_chunk_size = 4500
        chunks = [text[i:i+max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        
        translated_chunks = []
        for chunk in chunks:
            try:
                translated_chunk = translator.translate(chunk)
                translated_chunks.append(translated_chunk)
                time.sleep(0.1)  # Small delay to avoid rate limiting
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ±Ø¬Ù…Ø© Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù†Øµ: {e}")
                translated_chunks.append(chunk)  # Keep original if translation fails
        
        translated_text = ' '.join(translated_chunks)
        print(f"âœ… ØªÙ… ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù†Øµ Ù…Ù† {detected_lang} Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
        return translated_text
        
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù„ØºØ© Ø£Ùˆ Ø§Ù„ØªØ±Ø¬Ù…Ø©: {e}")
        return text

def extract_text_with_tesseract(pdf_path: str) -> str:
    """Extract text from PDF using Tesseract OCR with auto-correction and translation"""
    try:
        print(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Tesseract: {os.path.basename(pdf_path)}")
        
        doc = fitz.Document(pdf_path)
        full_text = ""
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            
            # Convert page to image
            mat = fitz.Matrix(2.0, 2.0)  # High resolution for better OCR
            pix = page.get_pixmap(matrix=mat)
            img_data = pix.tobytes("png")
            
            # Convert to PIL Image
            image = Image.open(io.BytesIO(img_data))
            img_array = np.array(image)
            
            # Preprocess image for better OCR
            if len(img_array.shape) == 3:
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            else:
                gray = img_array
            
            # Apply image processing for better OCR
            denoised = cv2.fastNlMeansDenoising(gray)
            
            # Use Tesseract to extract text
            try:
                # Try multiple configurations
                configs = [
                    '--oem 3 --psm 6',  # Standard
                    '--oem 3 --psm 4',  # Single column
                    '--oem 3 --psm 3',  # Fully automatic
                ]
                
                page_text = ""
                for config in configs:
                    try:
                        extracted_text = pytesseract.image_to_string(denoised, config=config)
                        if extracted_text.strip() and len(extracted_text.strip()) > 20:
                            page_text = extracted_text
                            break
                    except:
                        continue
                
                if page_text:
                    # Step 1: Auto-correct the text
                    corrected_text = correct_text(page_text)
                    
                    # Step 2: Detect language and translate to Arabic if needed
                    arabic_text = detect_and_translate_to_arabic(corrected_text)
                    
                    full_text += arabic_text + "\n\n"
                    print(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page_num + 1}")
                else:
                    print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page_num + 1}")
                    
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ OCR Ù„Ù„ØµÙØ­Ø© {page_num + 1}: {e}")
        
        doc.close()
        
        # Final cleaning
        final_text = clean_arabic_text(full_text)
        
        if final_text:
            print(f"ğŸ“ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ±Ø¬Ù…Ø© {len(final_text)} Ø­Ø±Ù Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©")
        else:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†Øµ Ù…ÙÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©")
        
        return final_text
        
    except Exception as e:
        error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© {pdf_path}: {e}"
        print(f"âŒ {error_msg}")
        return error_msg

def clean_arabic_text(text: str) -> str:
    """Clean and normalize Arabic text"""
    if not text or not text.strip():
        return ""
    
    try:
        # Normalize Unicode
        text = unicodedata.normalize('NFKC', text)
        
        # Remove unwanted characters but keep Arabic, numbers, and punctuation
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\u0660-\u0669\u06F0-\u06F9\u0020-\u007F\u00A0-\u00FF\n\r\t .,;:!?\-/Â°%()]+', ' ', text)
        
        # Fix spacing
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        return text.strip()
        
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {e}")
        return text.strip() if text else ""

def initialize_embedding_model():
    """Initialize Arabic-compatible embedding model"""
    global embedding_model
    
    try:
        print("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± Ù„Ù„Ø¨Ø­Ø«...")
        embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ± Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±: {e}")
        return False

def save_embeddings_and_documents(embeddings_array):
    """Save embeddings and documents to file"""
    global documents
    
    try:
        print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
        
        save_data = {
            'embeddings': embeddings_array,
            'documents': documents,
            'index_size': len(embeddings_array)
        }
        
        with open(EMBEDDINGS_FILE, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ {len(documents)} ÙˆØ«ÙŠÙ‚Ø© Ùˆ {len(embeddings_array)} ØªØ¶Ù…ÙŠÙ†")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª: {e}")
        return False

def load_embeddings_and_documents():
    """Load embeddings and documents from file"""
    global faiss_index, documents
    
    try:
        if not os.path.exists(EMBEDDINGS_FILE):
            print("ğŸ“ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù…Ø­ÙÙˆØ¸")
            return False
        
        print("ğŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©...")
        
        with open(EMBEDDINGS_FILE, 'rb') as f:
            save_data = pickle.load(f)
        
        documents = save_data['documents']
        embeddings = save_data['embeddings']
        dimension = embeddings.shape[1]
        
        faiss_index = faiss.IndexFlatIP(dimension)
        if len(embeddings.shape) > 1:
            faiss_index.add(embeddings.astype('float32'))
        else:
            faiss_index.add(embeddings.reshape(1, -1).astype('float32'))
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} ÙˆØ«ÙŠÙ‚Ø© Ùˆ {faiss_index.ntotal} ØªØ¶Ù…ÙŠÙ† Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª: {e}")
        return False

def smart_chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:
    """Smart chunking for text"""
    if not text or len(text.strip()) < 50:
        return []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        if end >= len(text):
            chunk = text[start:].strip()
            if len(chunk) > 50:
                chunks.append(chunk)
            break
        
        chunk = text[start:end]
        
        # Try to end at natural boundaries
        boundaries = ['.', '!', '?', '\n', ':', ';']
        for boundary in boundaries:
            pos = chunk.rfind(boundary)
            if pos > chunk_size // 2:
                end = start + pos + 1
                chunk = text[start:end]
                break
        
        chunk = chunk.strip()
        if len(chunk) > 50:
            chunks.append(chunk)
        
        start = end - overlap
        if start >= len(text):
            break
    
    return chunks

async def query_groq(prompt: str, system_prompt: str = None) -> str:
    """Query Groq API"""
    if not groq_client:
        return "âŒ Groq client not initialized"
    
    try:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=messages,
            temperature=0.1,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Groq: {e}")
        return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {str(e)}"

def search_documents(query: str, top_k: int = TOP_K) -> List[Dict]:
    """Search for relevant documents"""
    if faiss_index is None or not documents or embedding_model is None:
        return []
    
    try:
        query_embedding = embedding_model.encode([query])
        query_embedding = query_embedding.astype('float32')
        faiss.normalize_L2(query_embedding)
        
        scores, indices = faiss_index.search(query_embedding, top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if 0 <= idx < len(documents):
                normalized_score = max(0.0, min(1.0, float(score)))
                
                results.append({
                    'content': documents[idx]['content'],
                    'source': documents[idx]['source'],
                    'metadata': documents[idx]['metadata'],
                    'similarity_score': normalized_score,
                    'chunk_id': documents[idx]['chunk_id']
                })
        
        return results
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return []

async def information_collection_agent(user_message: str) -> Dict[str, str]:
    """Agent to collect business information from user"""
    
    system_prompt = """Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø´Ø±ÙƒØ§Øª ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ. 
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ù„Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†.

ÙŠØ¬Ø¨ Ø£Ù† ØªØ³Ø£Ù„ Ø¹Ù†:
1. Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø´Ø±ÙƒØ©: Ù†ÙˆØ¹ Ø§Ù„ÙƒÙŠØ§Ù† Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ (ÙØ±Ø¯ÙŠØ©ØŒ Ø°Ø§Øª Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ù…Ø­Ø¯ÙˆØ¯Ø©ØŒ Ù…Ø³Ø§Ù‡Ù…Ø©)
2. Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ©: Ø§Ù„Ø¥ÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ Ø±Ø£Ø³ Ø§Ù„Ù…Ø§Ù„ØŒ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠØ© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
3. Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ©: Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©ØŒ Ù†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø§Øª/Ø§Ù„Ù…Ù†ØªØ¬Ø§ØªØŒ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©
4. Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠ: Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¨Ù‚ØŒ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù‚Ø©
5. Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„ ÙˆØ§Ù„ÙˆØ«Ø§Ø¦Ù‚: Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©

Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙŠØ© ÙˆÙ…Ù‡Ù†ÙŠØ© ÙˆØ§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©."""

    prompt = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ©ØŒ Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:

Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„: {user_message}

Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªÙˆÙØ±Ø©ØŒ Ø«Ù… Ø§Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©.

Ù‚Ø¯Ù… Ø§Ù„Ø±Ø¯ ÙÙŠ Ø´ÙƒÙ„ JSON Ù…Ø¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØªØ§Ù„ÙŠØ©:
- collected_info: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙ… Ø¬Ù…Ø¹Ù‡Ø§ Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø©
- follow_up_questions: Ø£Ø³Ø¦Ù„Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
- next_steps: Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©"""

    response = await query_groq(prompt, system_prompt)
    
    try:
        # Try to parse JSON response
        import json
        parsed_response = json.loads(response)
        return parsed_response
    except:
        # If JSON parsing fails, return structured response
        return {
            "collected_info": {"general_inquiry": user_message},
            "follow_up_questions": response,
            "next_steps": "Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
        }

async def search_planning_agent(collected_info: Dict) -> List[SearchItem]:
    """Agent to plan search queries based on collected information"""
    
    system_prompt = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© 
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„.

Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ÙˆØ§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙŠ Ø³ØªØ³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©."""

    info_text = json.dumps(collected_info, ensure_ascii=False, indent=2)
    
    prompt = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„:

{info_text}

Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† 5-8 Ø¹Ù†Ø§ØµØ± Ø¨Ø­Ø« Ù…Ø­Ø¯Ø¯Ø© Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† ÙˆØ§Ù„Ù„ÙˆØ§Ø¦Ø­ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©.

Ù‚Ø¯Ù… Ø§Ù„Ø±Ø¯ ÙÙŠ Ø´ÙƒÙ„ JSON Ù…Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ØµØ±ØŒ ÙƒÙ„ Ø¹Ù†ØµØ± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
- query: Ù†Øµ Ø§Ù„Ø¨Ø­Ø«
- priority: Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© (1-5)
- category: ÙØ¦Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† (Ø¶Ø±ÙŠØ¨ÙŠØŒ ØªØ¬Ø§Ø±ÙŠØŒ Ø¹Ù…Ø§Ù„ÙŠØŒ Ø¥Ù„Ø®)

Ù…Ø«Ø§Ù„:
[
  {{"query": "ØªØ³Ø¬ÙŠÙ„ Ø´Ø±ÙƒØ© Ø°Ø§Øª Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ù…Ø­Ø¯ÙˆØ¯Ø©", "priority": 5, "category": "ØªØ¬Ø§Ø±ÙŠ"}},
  {{"query": "Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„ØµØºÙŠØ±Ø©", "priority": 4, "category": "Ø¶Ø±ÙŠØ¨ÙŠ"}}
]"""

    response = await query_groq(prompt, system_prompt)
    
    try:
        parsed_response = json.loads(response)
        search_items = []
        for item in parsed_response:
            search_items.append(SearchItem(
                query=item.get('query', ''),
                priority=item.get('priority', 3),
                category=item.get('category', 'Ø¹Ø§Ù…')
            ))
        return search_items
    except:
        # Fallback search items
        return [
            SearchItem(query="ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø´Ø±ÙƒØ§Øª", priority=5, category="ØªØ¬Ø§Ø±ÙŠ"),
            SearchItem(query="Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§Øª", priority=4, category="Ø¶Ø±ÙŠØ¨ÙŠ"),
            SearchItem(query="Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ", priority=3, category="Ø¹Ø§Ù…")
        ]

async def rag_search_agent(search_items: List[SearchItem]) -> List[RAGResult]:
    """Agent to perform RAG searches and summarize results"""
    
    rag_results = []
    
    for item in search_items:
        # Search for documents
        documents = search_documents(item.query, top_k=3)
        
        if documents:
            # Create context from documents
            context = "\n\n".join([doc['content'][:500] + "..." for doc in documents])
            
            # Summarize findings
            system_prompt = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ. Ù‚Ù… Ø¨ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙŠØ¯Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„."""
            
            prompt = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† "{item.query}"ØŒ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:

{context}

Ù‚Ø¯Ù… Ù…Ù„Ø®ØµØ§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…ÙÙŠØ¯Ø§Ù‹ Ù„Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©."""

            summary = await query_groq(prompt, system_prompt)
            
            rag_results.append(RAGResult(
                search_item=item.query,
                documents=documents,
                summary=summary
            ))
        else:
            rag_results.append(RAGResult(
                search_item=item.query,
                documents=[],
                summary="Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."
            ))
    
    return rag_results

async def recommendation_agent(collected_info: Dict, rag_results: List[RAGResult]) -> str:
    """Agent to provide final recommendations"""
    
    system_prompt = """Ø£Ù†Øª Ù…Ø­Ø§Ù…ÙŠ Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ. Ù‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ø´Ø§Ù…Ù„Ø© ÙˆÙ…ÙØµÙ„Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„ 
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© ÙˆÙ†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ."""

    info_text = json.dumps(collected_info, ensure_ascii=False, indent=2)
    results_text = "\n\n".join([f"Ø§Ù„Ø¨Ø­Ø«: {result.search_item}\nØ§Ù„Ù…Ù„Ø®Øµ: {result.summary}" for result in rag_results])
    
    prompt = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„:
{info_text}

Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:
{results_text}

Ù‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ø´Ø§Ù…Ù„Ø© ØªØªØ¶Ù…Ù†:
1. Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„ØªØ³Ø¬ÙŠÙ„
2. Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„Ø¶Ø±ÙŠØ¨ÙŠØ©
3. Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
4. Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©
5. Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª ÙˆØ§Ù„Ù†ØµØ§Ø¦Ø­ Ø§Ù„Ù…Ù‡Ù…Ø©

Ù‚Ø¯Ù… Ø§Ù„Ø±Ø¯ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø¸Ù…Ø© ÙˆÙˆØ§Ø¶Ø­Ø©."""

    recommendation = await query_groq(prompt, system_prompt)
    return recommendation

@app.on_event("startup")
async def startup_event():
    """Initialize the system"""
    global embedding_model, faiss_index, documents
    
    print("ğŸš€ ØªØ­Ù…ÙŠÙ„ Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…Ø¹ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø°ÙƒÙŠ...")
    
    # Initialize components
    groq_success = initialize_groq()
    spell_success = initialize_spell_checker()
    embedding_success = initialize_embedding_model()
    
    if not embedding_success:
        print("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ´ÙÙŠØ±")
        return
    
    # Try to load existing embeddings
    if load_embeddings_and_documents():
        print("ğŸ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ø¨Ù†Ø¬Ø§Ø­!")
        return
    
    # Process PDFs if no cached embeddings
    print("ğŸ“š Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©...")
    documents = []
    
    if not os.path.exists(PDF_FOLDER):
        print(f"âŒ Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {PDF_FOLDER}")
        return
    
    pdf_files = glob.glob(os.path.join(PDF_FOLDER, "*.pdf"))
    total_files = len(pdf_files)
    print(f"ğŸ“„ ÙˆØ¬Ø¯Øª {total_files} ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©")
    
    for file_index, pdf_path in enumerate(pdf_files, 1):
        filename = os.path.basename(pdf_path)
        print(f"ğŸ“– Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ({file_index}/{total_files}): {filename}")
        
        # Use Tesseract with auto-correction and translation
        text = extract_text_with_tesseract(pdf_path)
        
        if text and len(text.strip()) > 100:
            chunks = smart_chunk_text(text)
            if chunks:
                for j, chunk in enumerate(chunks):
                    documents.append({
                        'content': chunk,
                        'source': filename,
                        'chunk_id': j,
                        'metadata': f"{filename} - Ø§Ù„Ù‚Ø³Ù… {j+1}"
                    })
                print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(chunks)} Ù‚Ø³Ù…")
            else:
                documents.append({
                    'content': text.strip(),
                    'source': filename,
                    'chunk_id': 0,
                    'metadata': f"{filename} - Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„"
                })
                print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ÙƒÙ‚Ø³Ù… ÙˆØ§Ø­Ø¯")
        else:
            print(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ ÙƒØ§ÙÙ ÙÙŠ {filename}")
    
    print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…: {len(documents)}")
    
    # Create embeddings and FAISS index
    if documents and embedding_model:
        print("ğŸ” Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
        
        document_texts = [doc['content'] for doc in documents]
        try:
            document_embeddings = embedding_model.encode(document_texts, show_progress_bar=True)
            
            dimension = document_embeddings.shape[1]
            faiss_index = faiss.IndexFlatIP(dimension)
            
            faiss.normalize_L2(document_embeddings.astype('float32'))
            faiss_index.add(document_embeddings.astype('float32'))
            
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS Ø¨Ù€ {faiss_index.ntotal} ØªØ´ÙÙŠØ±")
            
            save_embeddings_and_documents(document_embeddings)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«: {e}")
    
    print("ğŸ‰ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­!")

@app.get("/api/health")
async def health_check():
    return {
        "status": "healthy",
        "documents_loaded": len(documents),
        "embedding_model_loaded": embedding_model is not None,
        "faiss_index_ready": faiss_index is not None,
        "groq_ready": groq_client is not None,
        "system_type": "Arabic Legal RAG with Agentic Workflow"
    }

@app.post("/api/chat", response_model=AgentWorkflowResponse)
async def chat_with_agent(message: ChatMessage):
    """Main chat endpoint with agentic workflow"""
    try:
        start_time = time.time()
        
        print(f"ğŸ’¬ Ø±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©: {message.message}")
        
        # Step 1: Information Collection
        print("ğŸ” Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª...")
        collected_info = await information_collection_agent(message.message)
        
        # Step 2: Search Planning
        print("ğŸ“‹ ØªØ®Ø·ÙŠØ· Ø§Ù„Ø¨Ø­Ø«...")
        search_items = await search_planning_agent(collected_info)
        
        # Step 3: RAG Search
        print("ğŸ” Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_results = await rag_search_agent(search_items)
        
        # Step 4: Final Recommendation
        print("ğŸ“ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª...")
        final_recommendation = await recommendation_agent(collected_info, rag_results)
        
        processing_time = time.time() - start_time
        
        return AgentWorkflowResponse(
            collected_info=collected_info,
            search_items=search_items,
            rag_results=rag_results,
            final_recommendation=final_recommendation,
            processing_time=processing_time
        )
        
    except Exception as e:
        return AgentWorkflowResponse(
            collected_info={},
            search_items=[],
            rag_results=[],
            final_recommendation="",
            processing_time=0,
            error=str(e)
        )

@app.post("/api/upload")
async def upload_document(file: UploadFile = File(...)):
    """Upload and process a new document"""
    try:
        if not file.filename.endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are supported")
        
        # Save uploaded file temporarily
        temp_path = f"/tmp/{file.filename}"
        with open(temp_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # Process the document
        text = extract_text_with_tesseract(temp_path)
        
        if text and len(text.strip()) > 100:
            chunks = smart_chunk_text(text)
            new_documents = []
            
            for j, chunk in enumerate(chunks):
                new_doc = {
                    'content': chunk,
                    'source': file.filename,
                    'chunk_id': j,
                    'metadata': f"{file.filename} - Ø§Ù„Ù‚Ø³Ù… {j+1}"
                }
                documents.append(new_doc)
                new_documents.append(new_doc)
            
            # Update embeddings
            if embedding_model and faiss_index:
                new_texts = [doc['content'] for doc in new_documents]
                new_embeddings = embedding_model.encode(new_texts)
                faiss.normalize_L2(new_embeddings.astype('float32'))
                faiss_index.add(new_embeddings.astype('float32'))
                
                # Save updated embeddings
                all_embeddings = np.vstack([
                    np.array([embedding_model.encode([doc['content']]) for doc in documents[:-len(new_documents)]]).squeeze(),
                    new_embeddings
                ])
                save_embeddings_and_documents(all_embeddings)
            
            # Clean up temp file
            os.remove(temp_path)
            
            return {
                "message": f"ØªÙ… Ø±ÙØ¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¨Ù†Ø¬Ø§Ø­. ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(chunks)} Ù‚Ø³Ù… Ù†ØµÙŠ.",
                "chunks_created": len(chunks),
                "filename": file.filename
            }
        else:
            os.remove(temp_path)
            raise HTTPException(status_code=400, detail="Could not extract sufficient text from the document")
            
    except Exception as e:
        return {"error": f"Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©: {str(e)}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)