{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Installing required packages for Arabic PDF RAG System...\n",
            "============================================================\n",
            "\n",
            "📥 Installing PyMuPDF...\n",
            "✅ Successfully installed: PyMuPDF\n",
            "\n",
            "📥 Installing sentence-transformers...\n",
            "✅ Successfully installed: sentence-transformers\n",
            "\n",
            "📥 Installing faiss-cpu...\n",
            "✅ Successfully installed: faiss-cpu\n",
            "\n",
            "📥 Installing numpy...\n",
            "✅ Successfully installed: numpy\n",
            "\n",
            "📥 Installing typing-extensions...\n",
            "✅ Successfully installed: typing-extensions\n",
            "\n",
            "============================================================\n",
            "✅ All packages installed successfully!\n",
            "🔄 Please restart the kernel and run the next cell.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# 📦 INSTALLATION CELL - Run this first!\n",
        "# Installing all required packages for Arabic PDF RAG System\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✅ Successfully installed: {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"❌ Failed to install {package}: {e}\")\n",
        "\n",
        "# List of required packages\n",
        "packages = [\n",
        "    \"PyMuPDF\",           # For PDF processing (correct package, not 'fitz')\n",
        "    \"sentence-transformers\",  # For Arabic embeddings\n",
        "    \"faiss-cpu\",         # For vector similarity search\n",
        "    \"numpy\",             # For numerical operations\n",
        "    \"typing-extensions\", # For type hints\n",
        "]\n",
        "\n",
        "print(\"🚀 Installing required packages for Arabic PDF RAG System...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for package in packages:\n",
        "    print(f\"\\n📥 Installing {package}...\")\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ All packages installed successfully!\")\n",
        "print(\"🔄 Please restart the kernel and run the next cell.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n",
            "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for PDF handling\u001b[39;00m\n",
            "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
            "\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n",
            "\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n",
            "\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n",
            "\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n",
            "\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n",
            "\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n",
            "\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n",
            "\u001b[1;32m     15\u001b[0m     CrossEncoder,\n",
            "\u001b[1;32m     16\u001b[0m     CrossEncoderModelCardData,\n",
            "\u001b[1;32m     17\u001b[0m     CrossEncoderTrainer,\n",
            "\u001b[1;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n",
            "\u001b[1;32m     19\u001b[0m )\n",
            "\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:5\u001b[0m\n",
            "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n",
            "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n",
            "\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
            "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n",
            "\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n",
            "\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainingArguments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderModelCardData\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     13\u001b[0m ]\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/trainer.py:22\u001b[0m\n",
            "\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator, SequentialEvaluator\n",
            "\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n",
            "\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_datasets_available, is_training_available\n",
            "\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/trainer.py:14\u001b[0m\n",
            "\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
            "\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSampler, ConcatDataset, DataLoader, RandomSampler\n",
            "\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\n",
            "\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m transformers_version\n",
            "\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2154\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
            "\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n",
            "\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m-> 2154\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   2155\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
            "\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2184\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
            "\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2182\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m-> 2182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   2183\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;32m   2184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/saher/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n",
            "\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:229\u001b[0m\n",
            "\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m accelerate_version\n",
            "\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState\n",
            "\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n",
            "\u001b[1;32m    230\u001b[0m     AutocastKwargs,\n",
            "\u001b[1;32m    231\u001b[0m     DistributedDataParallelKwargs,\n",
            "\u001b[1;32m    232\u001b[0m     DistributedType,\n",
            "\u001b[1;32m    233\u001b[0m     TorchTensorParallelPlugin,\n",
            "\u001b[1;32m    234\u001b[0m     load_fsdp_model,\n",
            "\u001b[1;32m    235\u001b[0m     load_fsdp_optimizer,\n",
            "\u001b[1;32m    236\u001b[0m     save_fsdp_model,\n",
            "\u001b[1;32m    237\u001b[0m     save_fsdp_optimizer,\n",
            "\u001b[1;32m    238\u001b[0m )\n",
            "\u001b[1;32m    240\u001b[0m DATA_SAMPLERS \u001b[38;5;241m=\u001b[39m [RandomSampler]\n",
            "\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(accelerate_version) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Streamlined Arabic PDF RAG System\n",
        "import os\n",
        "import glob\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🚀 Arabic PDF RAG System - Loading...\")\n",
        "\n",
        "# Configuration\n",
        "PDF_FOLDER = r\"D:\\NLP_S\\قوانين\"\n",
        "CHUNK_SIZE = 1000  # Increased chunk size\n",
        "OVERLAP = 100      # Overlap between chunks\n",
        "TOP_K = 3          # Number of results to retrieve\n",
        "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "def clean_arabic_text(text: str) -> str:\n",
        "    \"\"\"Clean Arabic text from OCR artifacts\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Basic cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Multiple newlines to single\n",
        "    text = re.sub(r'[\\.]{3,}', '...', text)  # Multiple dots\n",
        "    text = re.sub(r'[-]{2,}', '--', text)  # Multiple dashes\n",
        "    \n",
        "    # Remove page markers and noise\n",
        "    text = re.sub(r'Page \\d+', '', text)\n",
        "    text = re.sub(r'صفحة \\d+', '', text)\n",
        "    text = re.sub(r'^[-=_\\s]+$', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Fix Arabic punctuation spacing\n",
        "    text = re.sub(r'\\s*([،؛؟!])\\s*', r'\\1 ', text)\n",
        "    \n",
        "    # Remove single character noise\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF with OCR for scanned pages\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        \n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text()\n",
        "            \n",
        "            # If no text (scanned page), try OCR\n",
        "            if not text.strip():\n",
        "                try:\n",
        "                    # Convert page to image and use OCR\n",
        "                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # Higher resolution\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "                    \n",
        "                    # Try different text extraction methods for scanned content\n",
        "                    text = page.get_text(\"text\") or page.get_text(\"dict\")\n",
        "                    if isinstance(text, dict):\n",
        "                        # Extract from dict format\n",
        "                        extracted = \"\"\n",
        "                        for block in text.get(\"blocks\", []):\n",
        "                            if \"lines\" in block:\n",
        "                                for line in block[\"lines\"]:\n",
        "                                    for span in line[\"spans\"]:\n",
        "                                        extracted += span.get(\"text\", \"\") + \" \"\n",
        "                                extracted += \"\\n\"\n",
        "                        text = extracted\n",
        "                    \n",
        "                    if not text.strip():\n",
        "                        text = f\"[SCANNED PAGE {page_num+1} - TEXT NOT EXTRACTABLE]\"\n",
        "                except:\n",
        "                    text = f\"[ERROR EXTRACTING PAGE {page_num+1}]\"\n",
        "            \n",
        "            # Clean and add to full text\n",
        "            cleaned_text = clean_arabic_text(text)\n",
        "            full_text += cleaned_text + \"\\n\"\n",
        "        \n",
        "        doc.close()\n",
        "        return clean_arabic_text(full_text)\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error processing {pdf_path}: {e}\"\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:\n",
        "    \"\"\"Split text into chunks\"\"\"\n",
        "    if not text or len(text.strip()) < 100:\n",
        "        return []\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        \n",
        "        if end >= len(text):\n",
        "            chunk = text[start:].strip()\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "            break\n",
        "        \n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        # Try to end at sentence boundaries\n",
        "        for ending in ['.', '؟', '!', '؛', '\\n']:\n",
        "            pos = chunk.rfind(ending)\n",
        "            if pos > chunk_size // 2:\n",
        "                end = start + pos + 1\n",
        "                chunk = text[start:end]\n",
        "                break\n",
        "        \n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) > 100:\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        start = end - overlap\n",
        "        \n",
        "        if start >= len(text):\n",
        "            break\n",
        "    \n",
        "    return [c for c in chunks if len(c.strip()) > 100]\n",
        "\n",
        "# Process PDFs\n",
        "print(\"📚 Processing PDFs...\")\n",
        "documents = []\n",
        "pdf_files = glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
        "\n",
        "for pdf_path in pdf_files:\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    \n",
        "    if text and len(text.strip()) > 100:\n",
        "        chunks = chunk_text(text)\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            documents.append({\n",
        "                'content': chunk,\n",
        "                'source': filename,\n",
        "                'chunk_id': j,\n",
        "                'metadata': f\"{filename} - Chunk {j+1}\"\n",
        "            })\n",
        "        print(f\"✅ Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(f\"⚠️ No usable text from {filename}\")\n",
        "\n",
        "print(f\"Total chunks: {len(documents)}\")\n",
        "\n",
        "# Create embeddings and FAISS index\n",
        "print(\"🧮 Creating embeddings...\")\n",
        "model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "texts = [doc['content'] for doc in documents]\n",
        "if texts:\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    \n",
        "    # Create FAISS index for cosine similarity\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    \n",
        "    # Normalize for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"✅ Created FAISS index with {index.ntotal} vectors\")\n",
        "else:\n",
        "    print(\"❌ No documents to index\")\n",
        "    index = None\n",
        "\n",
        "# Search function\n",
        "def search_documents(query: str, top_k: int = TOP_K) -> List[Dict]:\n",
        "    \"\"\"Search for relevant documents\"\"\"\n",
        "    if index is None or not documents:\n",
        "        return []\n",
        "    \n",
        "    # Encode and search\n",
        "    query_embedding = model.encode([query])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
        "    \n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx < len(documents):\n",
        "            results.append({\n",
        "                'content': documents[idx]['content'],\n",
        "                'source': documents[idx]['source'],\n",
        "                'metadata': documents[idx]['metadata'],\n",
        "                'similarity_score': float(score)\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Interactive search\n",
        "print(\"✅ System ready! Use the search function below:\")\n",
        "\n",
        "def search_and_display(query: str):\n",
        "    \"\"\"Search and display results\"\"\"\n",
        "    if not query.strip():\n",
        "        print(\"❌ Please enter a question\")\n",
        "        return\n",
        "    \n",
        "    results = search_documents(query)\n",
        "    \n",
        "    if results:\n",
        "        print(f\"\\n📄 Top {len(results)} results for: {query}\\n\")\n",
        "        for i, doc in enumerate(results, 1):\n",
        "            print(f\"--- Result {i} ---\")\n",
        "            print(f\"📂 Source: {doc['source']}\")\n",
        "            print(f\"📊 Score: {doc['similarity_score']:.3f}\")\n",
        "            print(f\"📝 Content: {doc['content'][:400]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"❌ No relevant documents found.\")\n",
        "\n",
        "# Example usage:\n",
        "# search_and_display(\"ما هي الضرائب المفروضة على الشركات؟\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for PDF handling\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     CrossEncoder,\n\u001b[1;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainingArguments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderModelCardData\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/trainer.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator, SequentialEvaluator\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_datasets_available, is_training_available\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/trainer.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSampler, ConcatDataset, DataLoader, RandomSampler\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m transformers_version\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2154\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2154\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2155\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2184\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2182\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/miniconda3/envs/saher/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:229\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m accelerate_version\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     AutocastKwargs,\n\u001b[1;32m    231\u001b[0m     DistributedDataParallelKwargs,\n\u001b[1;32m    232\u001b[0m     DistributedType,\n\u001b[1;32m    233\u001b[0m     TorchTensorParallelPlugin,\n\u001b[1;32m    234\u001b[0m     load_fsdp_model,\n\u001b[1;32m    235\u001b[0m     load_fsdp_optimizer,\n\u001b[1;32m    236\u001b[0m     save_fsdp_model,\n\u001b[1;32m    237\u001b[0m     save_fsdp_optimizer,\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    240\u001b[0m DATA_SAMPLERS \u001b[38;5;241m=\u001b[39m [RandomSampler]\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(accelerate_version) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Streamlined Arabic PDF RAG System\n",
        "import os\n",
        "import glob\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🚀 Arabic PDF RAG System - Loading...\")\n",
        "\n",
        "# Configuration\n",
        "PDF_FOLDER = r\"D:\\NLP_S\\قوانين\"\n",
        "CHUNK_SIZE = 1000  # Increased chunk size\n",
        "OVERLAP = 100      # Overlap between chunks\n",
        "TOP_K = 3          # Number of results to retrieve\n",
        "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "def clean_arabic_text(text: str) -> str:\n",
        "    \"\"\"Clean Arabic text from OCR artifacts\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Basic cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Multiple newlines to single\n",
        "    text = re.sub(r'[\\.]{3,}', '...', text)  # Multiple dots\n",
        "    text = re.sub(r'[-]{2,}', '--', text)  # Multiple dashes\n",
        "    \n",
        "    # Remove page markers and noise\n",
        "    text = re.sub(r'Page \\d+', '', text)\n",
        "    text = re.sub(r'صفحة \\d+', '', text)\n",
        "    text = re.sub(r'^[-=_\\s]+$', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Fix Arabic punctuation spacing\n",
        "    text = re.sub(r'\\s*([،؛؟!])\\s*', r'\\1 ', text)\n",
        "    \n",
        "    # Remove single character noise\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF with OCR for scanned pages\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        \n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text()\n",
        "            \n",
        "            # If no text (scanned page), try OCR\n",
        "            if not text.strip():\n",
        "                try:\n",
        "                    # Convert page to image and use OCR\n",
        "                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # Higher resolution\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "                    \n",
        "                    # Try different text extraction methods for scanned content\n",
        "                    text = page.get_text(\"text\") or page.get_text(\"dict\")\n",
        "                    if isinstance(text, dict):\n",
        "                        # Extract from dict format\n",
        "                        extracted = \"\"\n",
        "                        for block in text.get(\"blocks\", []):\n",
        "                            if \"lines\" in block:\n",
        "                                for line in block[\"lines\"]:\n",
        "                                    for span in line[\"spans\"]:\n",
        "                                        extracted += span.get(\"text\", \"\") + \" \"\n",
        "                                extracted += \"\\n\"\n",
        "                        text = extracted\n",
        "                    \n",
        "                    if not text.strip():\n",
        "                        text = f\"[SCANNED PAGE {page_num+1} - TEXT NOT EXTRACTABLE]\"\n",
        "                except:\n",
        "                    text = f\"[ERROR EXTRACTING PAGE {page_num+1}]\"\n",
        "            \n",
        "            # Clean and add to full text\n",
        "            cleaned_text = clean_arabic_text(text)\n",
        "            full_text += cleaned_text + \"\\n\"\n",
        "        \n",
        "        doc.close()\n",
        "        return clean_arabic_text(full_text)\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error processing {pdf_path}: {e}\"\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:\n",
        "    \"\"\"Split text into chunks\"\"\"\n",
        "    if not text or len(text.strip()) < 100:\n",
        "        return []\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        \n",
        "        if end >= len(text):\n",
        "            chunk = text[start:].strip()\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "            break\n",
        "        \n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        # Try to end at sentence boundaries\n",
        "        for ending in ['.', '؟', '!', '؛', '\\n']:\n",
        "            pos = chunk.rfind(ending)\n",
        "            if pos > chunk_size // 2:\n",
        "                end = start + pos + 1\n",
        "                chunk = text[start:end]\n",
        "                break\n",
        "        \n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) > 100:\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        start = end - overlap\n",
        "        \n",
        "        if start >= len(text):\n",
        "            break\n",
        "    \n",
        "    return [c for c in chunks if len(c.strip()) > 100]\n",
        "\n",
        "# Process PDFs\n",
        "print(\"📚 Processing PDFs...\")\n",
        "documents = []\n",
        "pdf_files = glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
        "\n",
        "for pdf_path in pdf_files:\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    \n",
        "    if text and len(text.strip()) > 100:\n",
        "        chunks = chunk_text(text)\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            documents.append({\n",
        "                'content': chunk,\n",
        "                'source': filename,\n",
        "                'chunk_id': j,\n",
        "                'metadata': f\"{filename} - Chunk {j+1}\"\n",
        "            })\n",
        "        print(f\"✅ Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(f\"⚠️ No usable text from {filename}\")\n",
        "\n",
        "print(f\"Total chunks: {len(documents)}\")\n",
        "\n",
        "# Create embeddings and FAISS index\n",
        "print(\"🧮 Creating embeddings...\")\n",
        "model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "texts = [doc['content'] for doc in documents]\n",
        "if texts:\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    \n",
        "    # Create FAISS index for cosine similarity\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    \n",
        "    # Normalize for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"✅ Created FAISS index with {index.ntotal} vectors\")\n",
        "else:\n",
        "    print(\"❌ No documents to index\")\n",
        "    index = None\n",
        "\n",
        "# Search function\n",
        "def search_documents(query: str, top_k: int = TOP_K) -> List[Dict]:\n",
        "    \"\"\"Search for relevant documents\"\"\"\n",
        "    if index is None or not documents:\n",
        "        return []\n",
        "    \n",
        "    # Encode and search\n",
        "    query_embedding = model.encode([query])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
        "    \n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx < len(documents):\n",
        "            results.append({\n",
        "                'content': documents[idx]['content'],\n",
        "                'source': documents[idx]['source'],\n",
        "                'metadata': documents[idx]['metadata'],\n",
        "                'similarity_score': float(score)\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Interactive search\n",
        "print(\"✅ System ready! Use the search function below:\")\n",
        "\n",
        "def search_and_display(query: str):\n",
        "    \"\"\"Search and display results\"\"\"\n",
        "    if not query.strip():\n",
        "        print(\"❌ Please enter a question\")\n",
        "        return\n",
        "    \n",
        "    results = search_documents(query)\n",
        "    \n",
        "    if results:\n",
        "        print(f\"\\n📄 Top {len(results)} results for: {query}\\n\")\n",
        "        for i, doc in enumerate(results, 1):\n",
        "            print(f\"--- Result {i} ---\")\n",
        "            print(f\"📂 Source: {doc['source']}\")\n",
        "            print(f\"📊 Score: {doc['similarity_score']:.3f}\")\n",
        "            print(f\"📝 Content: {doc['content'][:400]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"❌ No relevant documents found.\")\n",
        "\n",
        "# Example usage:\n",
        "# search_and_display(\"ما هي الضرائب المفروضة على الشركات؟\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📄 Top 3 results for: ما هي الضرائب المفروضة على الشركات؟\n",
            "\n",
            "--- Result 1 ---\n",
            "📂 Source: law_no.30-2023.pdf\n",
            "📊 Score: 0.709\n",
            "📝 Content: ﺍﻟﺘﻲ ﺘﺤﺼل ﻋﻠﻴﻬﺎ ﺍﻟﺤﻜﻭﻤﺔ ﻭﻭﺤﺩﺍ ﺕ ﺍﻹﺩﺍﺭﺓ ﺍﻟﻤﺤﻠﻴﺔ ﻭﻏﻴﺭﻫﺎ ﻤﻥ ﺍﻷﺸـﺨﺎﺹ ﺍﻻﻋﺘﺒﺎﺭﻴﺔ ﺍﻟﻌﺎﻤﺔ ﻤﻥ ﻤﺼﺎﺩﺭ ﺨﺎﺭﺝ ﻤﺼﺭ. ﻣﺎﺩﺓ)٦٥ ﻣﻜﺮﺭ ً ﺍ(: ﺘﺨﻀﻊ ﻟﻠﻀﺭﻴﺒﺔ ﺒﺴﻌ ﺭ)٠١٪ ( ﺩﻭﻥ ﺨﺼﻡ ﺃﻴﺔ ﺘﻜﺎﻟﻴﻑ ﺘﻭﺯﻴﻌـﺎﺕ ﺍﻷﺭﺒـﺎﺡ ﺍﻟﺘـﻲ ﺘﺠﺭﻴﻬﺎ ﺸﺭﻜﺎﺕ ﺍﻷﻤﻭﺍل ﺃﻭ ﺸﺭﻜﺎﺕ ﺍﻷﺸﺨﺎﺹ، ﺒﻤﺎ ﻓﻲ ﺫﻟﻙ ﺍﻟﺸﺭﻜﺎﺕ ﺍﻟﻤﻘﺎﻤﺔ ﺒﻨﻅـﺎﻡ ﺍﻟﻤﻨﺎﻁﻕ ﺍﻻﻗﺘﺼﺎﺩﻴﺔ ﺫﺍﺕ ﺍﻟﻁ ﺒﻴﻌﺔ ﺍﻟﺨﺎﺼﺔ ﻟﻠﺸﺨﺹ ﺍﻟﻁ ﺒﻴﻌﻲ ﻏﻴـﺭ ﺍﻟﻤﻘـﻴﻡ ﻭﺍﻟـﺸﺨﺹ ﺍﻻﻋﺘﺒﺎﺭ ﻯ ﺍﻟﻤﻘﻴﻡ ﺃﻭ ﻏﻴﺭ ﺍﻟﻤﻘﻴﻡ ﺒﻤﺎ ﻓﻲ ﺫﻟﻙ ﺃﺭﺒﺎﺡ ﺍﻷﺸﺨﺎﺹ ﺍﻻﻋﺘﺒﺎﺭﻴﺔ ﻏﻴـ...\n",
            "\n",
            "--- Result 2 ---\n",
            "📂 Source: law_no.30-2023.pdf\n",
            "📊 Score: 0.697\n",
            "📝 Content: ﻤﻥ ﻤﺭﺍﺠﻌﺔ ﻟﻠﺘﺸﺭﻴﻌﺎﺕ ﻭﺍﻹﺠﺭﺍﺀﺍﺕ ﻭﺍﻟﺤﻭﺍﻓﺯ ﺍﻟﻀﺭﻴﺒﻴﺔ ﻭﺍﻟﺠﻤﺭﻜﻴـﺔ، ﻭﻗﻴـﺎﺱ ﺤﺎﻻﺕ ﺍﻻﻤﺘﺜﺎل ﺍﻟﻀﺭﻴﺒﻲ ﻟﻠﻤﺴﺘﺜﻤﺭﻴﻥ. صورة إلكترونية ال يعتد بها عند التداول اﻟﺟرﯾدة اﻟرﺳﻣﯾﺔ– اﻟﻌدد ٤٢) ﺗﺎﺑﻊ ( ﻓﻰ ٥١ ﯾوﻧﯾﺔ ﺳﻧﺔ ٣٢٠٢ ٧١ ٩- ﺍﻟﻘﻴﺎﺱ ﺍﻟﻤﺴﺘﻤﺭ ﻟﻤﺸﺭﻭﻋﺎﺕ ﺍﻹﺩﺍﺭﺓ ﺍﻟﻀﺭﻴﺒﻴﺔ ﻭﻤﺩﻯ ﺍﻋﺘﻤﺎﺩﻫﺎ ﻋﻠﻰ ﺍﻷﺴـﺎﺱ ﺍﻟﻔﻌﻠﻲ ﺍﻟﻌﺎﺩل ﻓﻲ ﺘﺤﺼﻴل ﺠﻤﻴﻊ ﺃﻨﻭﺍﻉ ﺍﻟﻀﺭﺍﺌﺏ ﻭﺍﻟﺠﻤـﺎﺭﻙ، ﻭﺇﺠﺭﺍﺀﺍﺘﻬـﺎ ﻟﺨﻔـﺽ ﺍﻟﻁﻌﻭﻥ ﻭﺍﻟﻤﻨﺎﺯﻋﺎﺕ ﺍﻟﻀﺭﻴﺒﻴﺔ ﺇﻟﻰ ﺃﻗل ﺤﺩ ﻤﻤﻜﻥ . ﻭﻴﺼﺩﺭ ﺒﺘﺸﻜﻴل ﺍﻟﻤﺠﻠﺱ...\n",
            "\n",
            "--- Result 3 ---\n",
            "📂 Source: law_no.5.of_.2025.pdf\n",
            "📊 Score: 0.673\n",
            "📝 Content: ﻤﻭل ﺃﻭ ﺍﻟﻤﻜﻠﻑ ﻋﻥ ﻜل ﻓﺘﺭﺓ ﻤﻥ ﺍﻟ ﻔﺘﺭﺍﺕ ﺍﻟﻀﺭﻴﺒﻴﺔ ﻤﺤـل ﺍﻟﻨـﺯﺍﻉ ﺩﻭﻥ ﺍﻹﺨﻼل ﺒﺴﺩﺍﺩ ﺍﻟﻀﺭﻴﺒﺔ ﺍﻟﻤﺴﺘﺤﻘﺔ ﺒﺎﻹﻗﺭﺍﺭ. اﻟﺠﺮ ﯾﺪة اﻟﺮﺳﻤﯿﺔ– اﻟﻌﺪد ٦ ﻣﻜﺮر)و( ﻓﻰ ٢١ ﻓﺒﺮاﯾﺮ ﺳﻨﺔ ٥٢٠٢ ٥ ٢- ﺃﺩﺍﺀ ﻀﺭﻴﺒﺔ ﺘﻌﺎﺩل ﻗﻴﻤﺔ ﺍﻟﻀﺭﻴﺒﺔ ﻭﺍﺠﺒﺔ ﺍﻷﺩﺍﺀ ﻤﻥ ﻭﺍﻗﻊ ﺁﺨـﺭ ﺍﺘﻔـﺎﻕ ﺴـﺎﺒﻕ ﻋﻠﻰ ﺍﻟﻔﺘﺭﺓ ﺃﻭ ﺍﻟﻔﺘﺭﺍﺕ ﺍﻟﻀﺭﻴﺒﻴﺔ ﻤﺤل ﺍﻟﻨﺯﺍﻉ ﻤﻀﺎﻓﹰ ﺎ ﺇﻟﻴﻬﺎ ﻨﺴﺒﺔ)٠٤٪ (، ﻭﺫﻟـﻙ ﻓـﻰ ﺍﻟﺤﺎﻻﺕ ﺍﻵﺘﻴﺔ : ) ﺃ ( ﻋﺩﻡ ﺘﻘﺩﻴﻡ ﺍﻹﻗﺭﺍ ﺭ ﺍﻟﻀﺭﻴﺒﻲ ﻋﻥ ﺍﻟﻔﺘﺭﺓ ﺃﻭ ﺍﻟﻔﺘﺭﺍﺕ ﺍﻟﻀﺭﻴﺒﻴﺔ ﻤﺤل ﺍﻟﻨﺯﺍﻉ . )ﺏ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 🔍 Search your documents here!\n",
        "# Replace the question below with your own Arabic question\n",
        "\n",
        "search_and_display(\"ما هي الضرائب المفروضة على الشركات؟\")\n",
        "\n",
        "# You can also try these examples:\n",
        "# search_and_display(\"كيف يتم حساب ضريبة القيمة المضافة؟\")\n",
        "# search_and_display(\"ما هي العقوبات على التأخير في دفع الضرائب؟\")\n",
        "# search_and_display(\"ما هي شروط الإعفاء الضريبي؟\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🤖 Enhanced Arabic PDF RAG System with Ollama Integration\n",
        "import subprocess\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "\n",
        "def clean_ollama_response(response: str) -> str:\n",
        "    \"\"\"\n",
        "    Keep response as-is, including thinking parts\n",
        "    \"\"\"\n",
        "    if not response:\n",
        "        return \"❌ لم يتم الحصول على إجابة من النموذج\"\n",
        "    \n",
        "    # Keep everything as-is, just basic cleanup\n",
        "    response = response.strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "def query_ollama(prompt: str, model: str = \"deepseek-r1:7b\") -> str:\n",
        "    \"\"\"\n",
        "    Query Ollama model using subprocess with improved response handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare the command\n",
        "        cmd = [\"ollama\", \"run\", model]\n",
        "        \n",
        "        # Start the process\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdin=subprocess.PIPE,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "            encoding='utf-8'\n",
        "        )\n",
        "        \n",
        "        # Send the prompt and get response\n",
        "        stdout, stderr = process.communicate(input=prompt, timeout=600)  # 5 minute timeout\n",
        "        \n",
        "        if process.returncode != 0:\n",
        "            return f\"❌ خطأ في استدعاء Ollama: {stderr}\"\n",
        "        \n",
        "        # Clean and return response\n",
        "        cleaned_response = clean_ollama_response(stdout)\n",
        "        \n",
        "        return cleaned_response\n",
        "        \n",
        "    except subprocess.TimeoutExpired:\n",
        "        return \"❌ انتهت مهلة الاستجابة. يرجى المحاولة مرة أخرى.\"\n",
        "    except FileNotFoundError:\n",
        "        return \"❌ Ollama غير موجود. يرجى التأكد من تثبيت Ollama وإضافته إلى PATH.\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ خطأ في التواصل مع Ollama: {str(e)}\"\n",
        "\n",
        "def create_rag_prompt(question: str, search_results: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Create a concise prompt for the LLM with retrieved context\n",
        "    \"\"\"\n",
        "    \n",
        "    # Collect relevant content (take only first 2-3 most relevant results to reduce size)\n",
        "    context_texts = []\n",
        "    for i, result in enumerate(search_results[:3], 1):  # Limit to top 3 results\n",
        "        # Truncate content to reduce size\n",
        "        content = result['content'][:500] + \"...\" if len(result['content']) > 500 else result['content']\n",
        "        context_texts.append(f\"المصدر {i}: {content}\")\n",
        "    \n",
        "    context = \"\\n---\\n\".join(context_texts)\n",
        "    \n",
        "    prompt = f\"\"\"أنت خبير قانوني مصري. أجب على السؤال بناءً على النصوص القانونية المرفقة فقط.\n",
        "\n",
        "السؤال: {question}\n",
        "\n",
        "النصوص القانونية:\n",
        "{context}\n",
        "\n",
        "التعليمات:\n",
        "- أجب بالعربية فقط\n",
        "- استخدم النصوص المرفقة فقط\n",
        "- اذكر المصدر والمادة القانونية\n",
        "- كن دقيقاً ومختصراً\n",
        "\n",
        "الإجابة:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def enhanced_search_and_answer(query: str):\n",
        "    \"\"\"\n",
        "    Simple search function that returns question, retrieved documents, and model answer\n",
        "    \"\"\"\n",
        "    if not query.strip():\n",
        "        return {\"error\": \"يرجى إدخال سؤال\"}\n",
        "    \n",
        "    # Get search results\n",
        "    results = search_documents(query, top_k=5)\n",
        "    \n",
        "    if not results:\n",
        "        return {\"error\": \"لم يتم العثور على مستندات ذات صلة\"}\n",
        "    \n",
        "    # Create comprehensive prompt\n",
        "    rag_prompt = create_rag_prompt(query, results)\n",
        "    \n",
        "    # Get LLM response\n",
        "    llm_response = query_ollama(rag_prompt)\n",
        "    \n",
        "    return {\n",
        "        'question': query,\n",
        "        'retrieved_documents': results,\n",
        "        'model_answer': llm_response\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "السؤال: ما هي الضرائب المفروضة على الشركات؟\n",
            "\n",
            "عدد المستندات المسترجعة: 5\n",
            "\n",
            "الإجابة: Thinking...\n",
            "Okay, so I need to answer the question about what taxes are imposed on companies using the provided legal texts. Let me go through each source one by one.\n",
            "\n",
            "Starting with Source 1, which is in Arabic and talks about how businesses are subject to a tax rate of up to 65% on their taxable income. It mentions that this applies to partnerships and corporations taxed under the general business tax law without exempting individuals from certain gains, especially those made through ordinary activities. So, the main point here seems to be the high tax rate of 65%.\n",
            "\n",
            "Moving on to Source 2, it discusses regulations concerning shares, investments, and income tax, as well as excise duties. It mentions that companies must file reports quarterly based on their income. Additionally, there are provisions about exemptions for small investors and certain types of businesses. So, from this source, the taxes include income tax, excise duties, and quarterly reporting requirements.\n",
            "\n",
            "Source 3 is in French but seems to discuss something related to business tax again. It talks about a decrease in the excise duty on some goods due to an increase in production costs. However, it also mentions that some companies are exempt from business tax under certain conditions. So here, the key taxes mentioned are excise duties and the possibility of exemptions for specific businesses.\n",
            "\n",
            "Putting this all together, the main taxes imposed on companies seem to be:\n",
            "\n",
            "1. Business Tax at a rate up to 65% as per Source 1.\n",
            "2. Income Tax and Excise Duties as discussed in Source 2.\n",
            "3. Exemptions from business tax under certain conditions as mentioned in Source 3.\n",
            "\n",
            "I think these are the primary taxes companies in Egypt are subject to based on the provided texts. Each source adds a different aspect, so I'll need to include all of them in my answer, referencing each source appropriately.\n",
            "...done thinking.\n",
            "\n",
            "Based on the analysis of the provided legal texts, the main taxes imposed on companies in Egypt are:\n",
            "\n",
            "1. **Business Tax**: Imposed at a rate up to 65% as per Article 65 of the Law No. 24/2009. This applies to partnerships and corporations taxed under the general business tax law.\n",
            "\n",
            "2. **Income Tax and Excise Duties**: Companies are required to pay these taxes, with specific provisions for small investors and certain types of businesses (Source 2).\n",
            "\n",
            "3. **Exemptions from Business Tax**: Certain businesses may be exempt from paying business tax if they meet specific conditions outlined in the relevant legal provisions.\n",
            "\n",
            "Each of these points is supported by references to the provided sources, ensuring that the information is accurate and based on the legal texts given.\n"
          ]
        }
      ],
      "source": [
        "# 🚀 Test the Enhanced RAG System with Ollama\n",
        "# Make sure Ollama is running and deepseek-r1:7b model is available\n",
        "\n",
        "# Simple test - returns only question, retrieved documents, and model answer\n",
        "result = enhanced_search_and_answer(\"ما هي الضرائب المفروضة على الشركات؟\")\n",
        "\n",
        "# Display the clean result\n",
        "if 'error' in result:\n",
        "    print(f\"خطأ: {result['error']}\")\n",
        "else:\n",
        "    print(f\"السؤال: {result['question']}\")\n",
        "    print(f\"\\nعدد المستندات المسترجعة: {len(result['retrieved_documents'])}\")\n",
        "    print(f\"\\nالإجابة: {result['model_answer']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "saher",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
