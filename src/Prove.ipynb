{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Installing required packages for Arabic PDF RAG System...\n",
            "============================================================\n",
            "\n",
            "ðŸ“¥ Installing PyMuPDF...\n",
            "âœ… Successfully installed: PyMuPDF\n",
            "\n",
            "ðŸ“¥ Installing sentence-transformers...\n",
            "âœ… Successfully installed: sentence-transformers\n",
            "\n",
            "ðŸ“¥ Installing faiss-cpu...\n",
            "âœ… Successfully installed: faiss-cpu\n",
            "\n",
            "ðŸ“¥ Installing numpy...\n",
            "âœ… Successfully installed: numpy\n",
            "\n",
            "ðŸ“¥ Installing typing-extensions...\n",
            "âœ… Successfully installed: typing-extensions\n",
            "\n",
            "============================================================\n",
            "âœ… All packages installed successfully!\n",
            "ðŸ”„ Please restart the kernel and run the next cell.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“¦ INSTALLATION CELL - Run this first!\n",
        "# Installing all required packages for Arabic PDF RAG System\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"âœ… Successfully installed: {package}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âŒ Failed to install {package}: {e}\")\n",
        "\n",
        "# List of required packages\n",
        "packages = [\n",
        "    \"PyMuPDF\",           # For PDF processing (correct package, not 'fitz')\n",
        "    \"sentence-transformers\",  # For Arabic embeddings\n",
        "    \"faiss-cpu\",         # For vector similarity search\n",
        "    \"numpy\",             # For numerical operations\n",
        "    \"typing-extensions\", # For type hints\n",
        "]\n",
        "\n",
        "print(\"ðŸš€ Installing required packages for Arabic PDF RAG System...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for package in packages:\n",
        "    print(f\"\\nðŸ“¥ Installing {package}...\")\n",
        "    install_package(package)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… All packages installed successfully!\")\n",
        "print(\"ðŸ”„ Please restart the kernel and run the next cell.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n",
            "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for PDF handling\u001b[39;00m\n",
            "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
            "\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
            "\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n",
            "\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n",
            "\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n",
            "\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n",
            "\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n",
            "\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n",
            "\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n",
            "\u001b[1;32m     15\u001b[0m     CrossEncoder,\n",
            "\u001b[1;32m     16\u001b[0m     CrossEncoderModelCardData,\n",
            "\u001b[1;32m     17\u001b[0m     CrossEncoderTrainer,\n",
            "\u001b[1;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n",
            "\u001b[1;32m     19\u001b[0m )\n",
            "\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:5\u001b[0m\n",
            "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n",
            "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n",
            "\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
            "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n",
            "\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n",
            "\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainingArguments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderModelCardData\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
            "\u001b[1;32m     13\u001b[0m ]\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/trainer.py:22\u001b[0m\n",
            "\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n",
            "\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator, SequentialEvaluator\n",
            "\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n",
            "\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_datasets_available, is_training_available\n",
            "\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/trainer.py:14\u001b[0m\n",
            "\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
            "\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSampler, ConcatDataset, DataLoader, RandomSampler\n",
            "\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\n",
            "\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m transformers_version\n",
            "\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2154\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
            "\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n",
            "\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m-> 2154\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   2155\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
            "\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2184\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
            "\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2182\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;32m-> 2182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;32m   2183\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[1;32m   2184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\n",
            "File \u001b[0;32m~/miniconda3/envs/saher/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n",
            "\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:229\u001b[0m\n",
            "\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m accelerate_version\n",
            "\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState\n",
            "\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n",
            "\u001b[1;32m    230\u001b[0m     AutocastKwargs,\n",
            "\u001b[1;32m    231\u001b[0m     DistributedDataParallelKwargs,\n",
            "\u001b[1;32m    232\u001b[0m     DistributedType,\n",
            "\u001b[1;32m    233\u001b[0m     TorchTensorParallelPlugin,\n",
            "\u001b[1;32m    234\u001b[0m     load_fsdp_model,\n",
            "\u001b[1;32m    235\u001b[0m     load_fsdp_optimizer,\n",
            "\u001b[1;32m    236\u001b[0m     save_fsdp_model,\n",
            "\u001b[1;32m    237\u001b[0m     save_fsdp_optimizer,\n",
            "\u001b[1;32m    238\u001b[0m )\n",
            "\u001b[1;32m    240\u001b[0m DATA_SAMPLERS \u001b[38;5;241m=\u001b[39m [RandomSampler]\n",
            "\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(accelerate_version) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Streamlined Arabic PDF RAG System\n",
        "import os\n",
        "import glob\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ðŸš€ Arabic PDF RAG System - Loading...\")\n",
        "\n",
        "# Configuration\n",
        "PDF_FOLDER = r\"D:\\NLP_S\\Ù‚ÙˆØ§Ù†ÙŠÙ†\"\n",
        "CHUNK_SIZE = 1000  # Increased chunk size\n",
        "OVERLAP = 100      # Overlap between chunks\n",
        "TOP_K = 3          # Number of results to retrieve\n",
        "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "def clean_arabic_text(text: str) -> str:\n",
        "    \"\"\"Clean Arabic text from OCR artifacts\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Basic cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Multiple newlines to single\n",
        "    text = re.sub(r'[\\.]{3,}', '...', text)  # Multiple dots\n",
        "    text = re.sub(r'[-]{2,}', '--', text)  # Multiple dashes\n",
        "    \n",
        "    # Remove page markers and noise\n",
        "    text = re.sub(r'Page \\d+', '', text)\n",
        "    text = re.sub(r'ØµÙØ­Ø© \\d+', '', text)\n",
        "    text = re.sub(r'^[-=_\\s]+$', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Fix Arabic punctuation spacing\n",
        "    text = re.sub(r'\\s*([ØŒØ›ØŸ!])\\s*', r'\\1 ', text)\n",
        "    \n",
        "    # Remove single character noise\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF with OCR for scanned pages\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        \n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text()\n",
        "            \n",
        "            # If no text (scanned page), try OCR\n",
        "            if not text.strip():\n",
        "                try:\n",
        "                    # Convert page to image and use OCR\n",
        "                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # Higher resolution\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "                    \n",
        "                    # Try different text extraction methods for scanned content\n",
        "                    text = page.get_text(\"text\") or page.get_text(\"dict\")\n",
        "                    if isinstance(text, dict):\n",
        "                        # Extract from dict format\n",
        "                        extracted = \"\"\n",
        "                        for block in text.get(\"blocks\", []):\n",
        "                            if \"lines\" in block:\n",
        "                                for line in block[\"lines\"]:\n",
        "                                    for span in line[\"spans\"]:\n",
        "                                        extracted += span.get(\"text\", \"\") + \" \"\n",
        "                                extracted += \"\\n\"\n",
        "                        text = extracted\n",
        "                    \n",
        "                    if not text.strip():\n",
        "                        text = f\"[SCANNED PAGE {page_num+1} - TEXT NOT EXTRACTABLE]\"\n",
        "                except:\n",
        "                    text = f\"[ERROR EXTRACTING PAGE {page_num+1}]\"\n",
        "            \n",
        "            # Clean and add to full text\n",
        "            cleaned_text = clean_arabic_text(text)\n",
        "            full_text += cleaned_text + \"\\n\"\n",
        "        \n",
        "        doc.close()\n",
        "        return clean_arabic_text(full_text)\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error processing {pdf_path}: {e}\"\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:\n",
        "    \"\"\"Split text into chunks\"\"\"\n",
        "    if not text or len(text.strip()) < 100:\n",
        "        return []\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        \n",
        "        if end >= len(text):\n",
        "            chunk = text[start:].strip()\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "            break\n",
        "        \n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        # Try to end at sentence boundaries\n",
        "        for ending in ['.', 'ØŸ', '!', 'Ø›', '\\n']:\n",
        "            pos = chunk.rfind(ending)\n",
        "            if pos > chunk_size // 2:\n",
        "                end = start + pos + 1\n",
        "                chunk = text[start:end]\n",
        "                break\n",
        "        \n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) > 100:\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        start = end - overlap\n",
        "        \n",
        "        if start >= len(text):\n",
        "            break\n",
        "    \n",
        "    return [c for c in chunks if len(c.strip()) > 100]\n",
        "\n",
        "# Process PDFs\n",
        "print(\"ðŸ“š Processing PDFs...\")\n",
        "documents = []\n",
        "pdf_files = glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
        "\n",
        "for pdf_path in pdf_files:\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    \n",
        "    if text and len(text.strip()) > 100:\n",
        "        chunks = chunk_text(text)\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            documents.append({\n",
        "                'content': chunk,\n",
        "                'source': filename,\n",
        "                'chunk_id': j,\n",
        "                'metadata': f\"{filename} - Chunk {j+1}\"\n",
        "            })\n",
        "        print(f\"âœ… Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ No usable text from {filename}\")\n",
        "\n",
        "print(f\"Total chunks: {len(documents)}\")\n",
        "\n",
        "# Create embeddings and FAISS index\n",
        "print(\"ðŸ§® Creating embeddings...\")\n",
        "model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "texts = [doc['content'] for doc in documents]\n",
        "if texts:\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    \n",
        "    # Create FAISS index for cosine similarity\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    \n",
        "    # Normalize for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"âœ… Created FAISS index with {index.ntotal} vectors\")\n",
        "else:\n",
        "    print(\"âŒ No documents to index\")\n",
        "    index = None\n",
        "\n",
        "# Search function\n",
        "def search_documents(query: str, top_k: int = TOP_K) -> List[Dict]:\n",
        "    \"\"\"Search for relevant documents\"\"\"\n",
        "    if index is None or not documents:\n",
        "        return []\n",
        "    \n",
        "    # Encode and search\n",
        "    query_embedding = model.encode([query])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
        "    \n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx < len(documents):\n",
        "            results.append({\n",
        "                'content': documents[idx]['content'],\n",
        "                'source': documents[idx]['source'],\n",
        "                'metadata': documents[idx]['metadata'],\n",
        "                'similarity_score': float(score)\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Interactive search\n",
        "print(\"âœ… System ready! Use the search function below:\")\n",
        "\n",
        "def search_and_display(query: str):\n",
        "    \"\"\"Search and display results\"\"\"\n",
        "    if not query.strip():\n",
        "        print(\"âŒ Please enter a question\")\n",
        "        return\n",
        "    \n",
        "    results = search_documents(query)\n",
        "    \n",
        "    if results:\n",
        "        print(f\"\\nðŸ“„ Top {len(results)} results for: {query}\\n\")\n",
        "        for i, doc in enumerate(results, 1):\n",
        "            print(f\"--- Result {i} ---\")\n",
        "            print(f\"ðŸ“‚ Source: {doc['source']}\")\n",
        "            print(f\"ðŸ“Š Score: {doc['similarity_score']:.3f}\")\n",
        "            print(f\"ðŸ“ Content: {doc['content'][:400]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"âŒ No relevant documents found.\")\n",
        "\n",
        "# Example usage:\n",
        "# search_and_display(\"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„Ù…ÙØ±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§ØªØŸ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m  \u001b[38;5;66;03m# PyMuPDF for PDF handling\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/__init__.py:14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     CrossEncoder,\n\u001b[1;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[1;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderTrainingArguments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoderModelCardData\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/trainer.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator, SequentialEvaluator\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_datasets_available, is_training_available\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/trainer.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchSampler, ConcatDataset, DataLoader, RandomSampler\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m transformers_version\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_collator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollator\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2154\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2154\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2155\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2184\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2182\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2184\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[0;32m~/miniconda3/envs/saher/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:229\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m accelerate_version\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     AutocastKwargs,\n\u001b[1;32m    231\u001b[0m     DistributedDataParallelKwargs,\n\u001b[1;32m    232\u001b[0m     DistributedType,\n\u001b[1;32m    233\u001b[0m     TorchTensorParallelPlugin,\n\u001b[1;32m    234\u001b[0m     load_fsdp_model,\n\u001b[1;32m    235\u001b[0m     load_fsdp_optimizer,\n\u001b[1;32m    236\u001b[0m     save_fsdp_model,\n\u001b[1;32m    237\u001b[0m     save_fsdp_optimizer,\n\u001b[1;32m    238\u001b[0m )\n\u001b[1;32m    240\u001b[0m DATA_SAMPLERS \u001b[38;5;241m=\u001b[39m [RandomSampler]\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(accelerate_version) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TorchTensorParallelPlugin' from 'accelerate.utils' (/home/nu2/miniconda3/envs/saher/lib/python3.10/site-packages/accelerate/utils/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Streamlined Arabic PDF RAG System\n",
        "import os\n",
        "import glob\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"ðŸš€ Arabic PDF RAG System - Loading...\")\n",
        "\n",
        "# Configuration\n",
        "PDF_FOLDER = r\"D:\\NLP_S\\Ù‚ÙˆØ§Ù†ÙŠÙ†\"\n",
        "CHUNK_SIZE = 1000  # Increased chunk size\n",
        "OVERLAP = 100      # Overlap between chunks\n",
        "TOP_K = 3          # Number of results to retrieve\n",
        "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "def clean_arabic_text(text: str) -> str:\n",
        "    \"\"\"Clean Arabic text from OCR artifacts\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Basic cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Multiple newlines to single\n",
        "    text = re.sub(r'[\\.]{3,}', '...', text)  # Multiple dots\n",
        "    text = re.sub(r'[-]{2,}', '--', text)  # Multiple dashes\n",
        "    \n",
        "    # Remove page markers and noise\n",
        "    text = re.sub(r'Page \\d+', '', text)\n",
        "    text = re.sub(r'ØµÙØ­Ø© \\d+', '', text)\n",
        "    text = re.sub(r'^[-=_\\s]+$', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Fix Arabic punctuation spacing\n",
        "    text = re.sub(r'\\s*([ØŒØ›ØŸ!])\\s*', r'\\1 ', text)\n",
        "    \n",
        "    # Remove single character noise\n",
        "    text = re.sub(r'\\b[a-zA-Z]\\b', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extract text from PDF with OCR for scanned pages\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        full_text = \"\"\n",
        "        \n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text()\n",
        "            \n",
        "            # If no text (scanned page), try OCR\n",
        "            if not text.strip():\n",
        "                try:\n",
        "                    # Convert page to image and use OCR\n",
        "                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # Higher resolution\n",
        "                    img_data = pix.tobytes(\"png\")\n",
        "                    \n",
        "                    # Try different text extraction methods for scanned content\n",
        "                    text = page.get_text(\"text\") or page.get_text(\"dict\")\n",
        "                    if isinstance(text, dict):\n",
        "                        # Extract from dict format\n",
        "                        extracted = \"\"\n",
        "                        for block in text.get(\"blocks\", []):\n",
        "                            if \"lines\" in block:\n",
        "                                for line in block[\"lines\"]:\n",
        "                                    for span in line[\"spans\"]:\n",
        "                                        extracted += span.get(\"text\", \"\") + \" \"\n",
        "                                extracted += \"\\n\"\n",
        "                        text = extracted\n",
        "                    \n",
        "                    if not text.strip():\n",
        "                        text = f\"[SCANNED PAGE {page_num+1} - TEXT NOT EXTRACTABLE]\"\n",
        "                except:\n",
        "                    text = f\"[ERROR EXTRACTING PAGE {page_num+1}]\"\n",
        "            \n",
        "            # Clean and add to full text\n",
        "            cleaned_text = clean_arabic_text(text)\n",
        "            full_text += cleaned_text + \"\\n\"\n",
        "        \n",
        "        doc.close()\n",
        "        return clean_arabic_text(full_text)\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error processing {pdf_path}: {e}\"\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP) -> List[str]:\n",
        "    \"\"\"Split text into chunks\"\"\"\n",
        "    if not text or len(text.strip()) < 100:\n",
        "        return []\n",
        "    \n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        \n",
        "        if end >= len(text):\n",
        "            chunk = text[start:].strip()\n",
        "            if len(chunk) > 100:\n",
        "                chunks.append(chunk)\n",
        "            break\n",
        "        \n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        # Try to end at sentence boundaries\n",
        "        for ending in ['.', 'ØŸ', '!', 'Ø›', '\\n']:\n",
        "            pos = chunk.rfind(ending)\n",
        "            if pos > chunk_size // 2:\n",
        "                end = start + pos + 1\n",
        "                chunk = text[start:end]\n",
        "                break\n",
        "        \n",
        "        chunk = chunk.strip()\n",
        "        if len(chunk) > 100:\n",
        "            chunks.append(chunk)\n",
        "        \n",
        "        start = end - overlap\n",
        "        \n",
        "        if start >= len(text):\n",
        "            break\n",
        "    \n",
        "    return [c for c in chunks if len(c.strip()) > 100]\n",
        "\n",
        "# Process PDFs\n",
        "print(\"ðŸ“š Processing PDFs...\")\n",
        "documents = []\n",
        "pdf_files = glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\"))\n",
        "\n",
        "for pdf_path in pdf_files:\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"Processing: {filename}\")\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    \n",
        "    if text and len(text.strip()) > 100:\n",
        "        chunks = chunk_text(text)\n",
        "        for j, chunk in enumerate(chunks):\n",
        "            documents.append({\n",
        "                'content': chunk,\n",
        "                'source': filename,\n",
        "                'chunk_id': j,\n",
        "                'metadata': f\"{filename} - Chunk {j+1}\"\n",
        "            })\n",
        "        print(f\"âœ… Created {len(chunks)} chunks\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ No usable text from {filename}\")\n",
        "\n",
        "print(f\"Total chunks: {len(documents)}\")\n",
        "\n",
        "# Create embeddings and FAISS index\n",
        "print(\"ðŸ§® Creating embeddings...\")\n",
        "model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "\n",
        "texts = [doc['content'] for doc in documents]\n",
        "if texts:\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    \n",
        "    # Create FAISS index for cosine similarity\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    \n",
        "    # Normalize for cosine similarity\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    \n",
        "    print(f\"âœ… Created FAISS index with {index.ntotal} vectors\")\n",
        "else:\n",
        "    print(\"âŒ No documents to index\")\n",
        "    index = None\n",
        "\n",
        "# Search function\n",
        "def search_documents(query: str, top_k: int = TOP_K) -> List[Dict]:\n",
        "    \"\"\"Search for relevant documents\"\"\"\n",
        "    if index is None or not documents:\n",
        "        return []\n",
        "    \n",
        "    # Encode and search\n",
        "    query_embedding = model.encode([query])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    scores, indices = index.search(query_embedding.astype('float32'), top_k)\n",
        "    \n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx < len(documents):\n",
        "            results.append({\n",
        "                'content': documents[idx]['content'],\n",
        "                'source': documents[idx]['source'],\n",
        "                'metadata': documents[idx]['metadata'],\n",
        "                'similarity_score': float(score)\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Interactive search\n",
        "print(\"âœ… System ready! Use the search function below:\")\n",
        "\n",
        "def search_and_display(query: str):\n",
        "    \"\"\"Search and display results\"\"\"\n",
        "    if not query.strip():\n",
        "        print(\"âŒ Please enter a question\")\n",
        "        return\n",
        "    \n",
        "    results = search_documents(query)\n",
        "    \n",
        "    if results:\n",
        "        print(f\"\\nðŸ“„ Top {len(results)} results for: {query}\\n\")\n",
        "        for i, doc in enumerate(results, 1):\n",
        "            print(f\"--- Result {i} ---\")\n",
        "            print(f\"ðŸ“‚ Source: {doc['source']}\")\n",
        "            print(f\"ðŸ“Š Score: {doc['similarity_score']:.3f}\")\n",
        "            print(f\"ðŸ“ Content: {doc['content'][:400]}...\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"âŒ No relevant documents found.\")\n",
        "\n",
        "# Example usage:\n",
        "# search_and_display(\"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„Ù…ÙØ±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§ØªØŸ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“„ Top 3 results for: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„Ù…ÙØ±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§ØªØŸ\n",
            "\n",
            "--- Result 1 ---\n",
            "ðŸ“‚ Source: law_no.30-2023.pdf\n",
            "ðŸ“Š Score: 0.709\n",
            "ðŸ“ Content: ïºï»Ÿïº˜ï»² ïº˜ïº¤ïº¼Ù„ ï»‹ï» ï»´ï»¬ïºŽ ïºï»Ÿïº¤ï»œï»­ï»¤ïº” ï»­ï»­ïº¤ïº©ïº ïº• ïºï»¹ïº©ïºïº­ïº“ ïºï»Ÿï»¤ïº¤ï» ï»´ïº” ï»­ï»ï»´ïº­ï»«ïºŽ ï»¤ï»¥ ïºï»·ïº¸Ù€ïº¨ïºŽïº¹ ïºï»»ï»‹ïº˜ïº’ïºŽïº­ï»´ïº” ïºï»Ÿï»ŒïºŽï»¤ïº” ï»¤ï»¥ ï»¤ïº¼ïºŽïº©ïº­ ïº¨ïºŽïº­ïº ï»¤ïº¼ïº­. ï»£ïºŽïº©ïº“)Ù¦Ù¥ ï»£ï»œïº®ïº­ Ù‹ ïº(: ïº˜ïº¨ï»€ï»Š ï»Ÿï» ï»€ïº­ï»´ïº’ïº” ïº’ïº´ï»Œ ïº­)Ù Ù¡Ùª ( ïº©ï»­ï»¥ ïº¨ïº¼ï»¡ ïºƒï»´ïº” ïº˜ï»œïºŽï»Ÿï»´ï»‘ ïº˜ï»­ïº¯ï»´ï»ŒÙ€ïºŽïº• ïºï»·ïº­ïº’Ù€ïºŽïº¡ ïºï»Ÿïº˜Ù€ï»² ïº˜ïº ïº­ï»´ï»¬ïºŽ ïº¸ïº­ï»œïºŽïº• ïºï»·ï»¤ï»­ïºÙ„ ïºƒï»­ ïº¸ïº­ï»œïºŽïº• ïºï»·ïº¸ïº¨ïºŽïº¹ØŒ ïº’ï»¤ïºŽ ï»“ï»² ïº«ï»Ÿï»™ ïºï»Ÿïº¸ïº­ï»œïºŽïº• ïºï»Ÿï»¤ï»˜ïºŽï»¤ïº” ïº’ï»¨ï»…Ù€ïºŽï»¡ ïºï»Ÿï»¤ï»¨ïºŽï»ï»• ïºï»»ï»—ïº˜ïº¼ïºŽïº©ï»´ïº” ïº«ïºïº• ïºï»Ÿï» ïº’ï»´ï»Œïº” ïºï»Ÿïº¨ïºŽïº¼ïº” ï»Ÿï» ïº¸ïº¨ïº¹ ïºï»Ÿï» ïº’ï»´ï»Œï»² ï»ï»´Ù€ïº­ ïºï»Ÿï»¤ï»˜Ù€ï»´ï»¡ ï»­ïºï»ŸÙ€ïº¸ïº¨ïº¹ ïºï»»ï»‹ïº˜ïº’ïºŽïº­ ï»¯ ïºï»Ÿï»¤ï»˜ï»´ï»¡ ïºƒï»­ ï»ï»´ïº­ ïºï»Ÿï»¤ï»˜ï»´ï»¡ ïº’ï»¤ïºŽ ï»“ï»² ïº«ï»Ÿï»™ ïºƒïº­ïº’ïºŽïº¡ ïºï»·ïº¸ïº¨ïºŽïº¹ ïºï»»ï»‹ïº˜ïº’ïºŽïº­ï»´ïº” ï»ï»´Ù€...\n",
            "\n",
            "--- Result 2 ---\n",
            "ðŸ“‚ Source: law_no.30-2023.pdf\n",
            "ðŸ“Š Score: 0.697\n",
            "ðŸ“ Content: ï»¤ï»¥ ï»¤ïº­ïºïº ï»Œïº” ï»Ÿï» ïº˜ïº¸ïº­ï»´ï»ŒïºŽïº• ï»­ïºï»¹ïº ïº­ïºïº€ïºïº• ï»­ïºï»Ÿïº¤ï»­ïºï»“ïº¯ ïºï»Ÿï»€ïº­ï»´ïº’ï»´ïº” ï»­ïºï»Ÿïº ï»¤ïº­ï»œï»´Ù€ïº”ØŒ ï»­ï»—ï»´Ù€ïºŽïº± ïº¤ïºŽï»»ïº• ïºï»»ï»¤ïº˜ïºœïºŽÙ„ ïºï»Ÿï»€ïº­ï»´ïº’ï»² ï»Ÿï» ï»¤ïº´ïº˜ïºœï»¤ïº­ï»´ï»¥. ØµÙˆØ±Ø© Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø§Ù„ ÙŠØ¹ØªØ¯ Ø¨Ù‡Ø§ Ø¹Ù†Ø¯ Ø§Ù„ØªØ¯Ø§ÙˆÙ„ Ø§ï»ŸïºŸØ±ï¯¾Ø¯Ø© Ø§ï»ŸØ±ïº³ï»£ï¯¾ïº”â€“ Ø§ï»Ÿï»ŒØ¯Ø¯ Ù¤Ù¢) ïº—ïºŽïº‘ï»Š ( ï»“ï»° Ù¥Ù¡ ï¯¾Ùˆï»§ï¯¾ïº” ïº³ï»§ïº” Ù£Ù¢Ù Ù¢ Ù§Ù¡ Ù©- ïºï»Ÿï»˜ï»´ïºŽïº± ïºï»Ÿï»¤ïº´ïº˜ï»¤ïº­ ï»Ÿï»¤ïº¸ïº­ï»­ï»‹ïºŽïº• ïºï»¹ïº©ïºïº­ïº“ ïºï»Ÿï»€ïº­ï»´ïº’ï»´ïº” ï»­ï»¤ïº©ï»¯ ïºï»‹ïº˜ï»¤ïºŽïº©ï»«ïºŽ ï»‹ï» ï»° ïºï»·ïº´Ù€ïºŽïº± ïºï»Ÿï»”ï»Œï» ï»² ïºï»Ÿï»ŒïºŽïº©Ù„ ï»“ï»² ïº˜ïº¤ïº¼ï»´Ù„ ïº ï»¤ï»´ï»Š ïºƒï»¨ï»­ïºï»‰ ïºï»Ÿï»€ïº­ïºïºŒïº ï»­ïºï»Ÿïº ï»¤Ù€ïºŽïº­ï»™ØŒ ï»­ïº‡ïº ïº­ïºïº€ïºïº˜ï»¬Ù€ïºŽ ï»Ÿïº¨ï»”Ù€ïº½ ïºï»Ÿï»ï»Œï»­ï»¥ ï»­ïºï»Ÿï»¤ï»¨ïºŽïº¯ï»‹ïºŽïº• ïºï»Ÿï»€ïº­ï»´ïº’ï»´ïº” ïº‡ï»Ÿï»° ïºƒï»—Ù„ ïº¤ïº© ï»¤ï»¤ï»œï»¥ . ï»­ï»´ïº¼ïº©ïº­ ïº’ïº˜ïº¸ï»œï»´Ù„ ïºï»Ÿï»¤ïº ï» ïº±...\n",
            "\n",
            "--- Result 3 ---\n",
            "ðŸ“‚ Source: law_no.5.of_.2025.pdf\n",
            "ðŸ“Š Score: 0.673\n",
            "ðŸ“ Content: ï»¤ï»­Ù„ ïºƒï»­ ïºï»Ÿï»¤ï»œï» ï»‘ ï»‹ï»¥ ï»œÙ„ ï»“ïº˜ïº­ïº“ ï»¤ï»¥ ïºï»Ÿ ï»”ïº˜ïº­ïºïº• ïºï»Ÿï»€ïº­ï»´ïº’ï»´ïº” ï»¤ïº¤Ù€Ù„ ïºï»Ÿï»¨Ù€ïº¯ïºï»‰ ïº©ï»­ï»¥ ïºï»¹ïº¨ï»¼Ù„ ïº’ïº´ïº©ïºïº© ïºï»Ÿï»€ïº­ï»´ïº’ïº” ïºï»Ÿï»¤ïº´ïº˜ïº¤ï»˜ïº” ïº’ïºŽï»¹ï»—ïº­ïºïº­. Ø§ï»Ÿïº ïº® ï¯¾ïºªØ© Ø§ï»Ÿïº®ïº³ï»¤ï¯¿ïº”â€“ Ø§ï»Ÿï»ŒïºªØ¯ Ù¦ ï»£ï»œïº®Ø±)Ùˆ( ï»“ï»° Ù¢Ù¡ ï»“ïº’ïº®Ø§ï¯¾ïº® ïº³ï»¨ïº” Ù¥Ù¢Ù Ù¢ Ù¥ Ù¢- ïºƒïº©ïºïº€ ï»€ïº­ï»´ïº’ïº” ïº˜ï»ŒïºŽïº©Ù„ ï»—ï»´ï»¤ïº” ïºï»Ÿï»€ïº­ï»´ïº’ïº” ï»­ïºïº ïº’ïº” ïºï»·ïº©ïºïº€ ï»¤ï»¥ ï»­ïºï»—ï»Š ïºïº¨Ù€ïº­ ïºïº˜ï»”Ù€ïºŽï»• ïº´Ù€ïºŽïº’ï»• ï»‹ï» ï»° ïºï»Ÿï»”ïº˜ïº­ïº“ ïºƒï»­ ïºï»Ÿï»”ïº˜ïº­ïºïº• ïºï»Ÿï»€ïº­ï»´ïº’ï»´ïº” ï»¤ïº¤Ù„ ïºï»Ÿï»¨ïº¯ïºï»‰ ï»¤ï»€ïºŽï»“ï¹° ïºŽ ïº‡ï»Ÿï»´ï»¬ïºŽ ï»¨ïº´ïº’ïº”)Ù Ù¤Ùª (ØŒ ï»­ïº«ï»ŸÙ€ï»™ ï»“Ù€ï»° ïºï»Ÿïº¤ïºŽï»»ïº• ïºï»µïº˜ï»´ïº” : ) ïºƒ ( ï»‹ïº©ï»¡ ïº˜ï»˜ïº©ï»´ï»¡ ïºï»¹ï»—ïº­ïº ïº­ ïºï»Ÿï»€ïº­ï»´ïº’ï»² ï»‹ï»¥ ïºï»Ÿï»”ïº˜ïº­ïº“ ïºƒï»­ ïºï»Ÿï»”ïº˜ïº­ïºïº• ïºï»Ÿï»€ïº­ï»´ïº’ï»´ïº” ï»¤ïº¤Ù„ ïºï»Ÿï»¨ïº¯ïºï»‰ . )ïº...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ðŸ” Search your documents here!\n",
        "# Replace the question below with your own Arabic question\n",
        "\n",
        "search_and_display(\"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„Ù…ÙØ±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§ØªØŸ\")\n",
        "\n",
        "# You can also try these examples:\n",
        "# search_and_display(\"ÙƒÙŠÙ ÙŠØªÙ… Ø­Ø³Ø§Ø¨ Ø¶Ø±ÙŠØ¨Ø© Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©ØŸ\")\n",
        "# search_and_display(\"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ØªØ£Ø®ÙŠØ± ÙÙŠ Ø¯ÙØ¹ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ØŸ\")\n",
        "# search_and_display(\"Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ø¥Ø¹ÙØ§Ø¡ Ø§Ù„Ø¶Ø±ÙŠØ¨ÙŠØŸ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸ¤– Enhanced Arabic PDF RAG System with Ollama Integration\n",
        "import subprocess\n",
        "import json\n",
        "import sys\n",
        "import re\n",
        "\n",
        "def clean_ollama_response(response: str) -> str:\n",
        "    \"\"\"\n",
        "    Keep response as-is, including thinking parts\n",
        "    \"\"\"\n",
        "    if not response:\n",
        "        return \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\"\n",
        "    \n",
        "    # Keep everything as-is, just basic cleanup\n",
        "    response = response.strip()\n",
        "    \n",
        "    return response\n",
        "\n",
        "def query_ollama(prompt: str, model: str = \"deepseek-r1:7b\") -> str:\n",
        "    \"\"\"\n",
        "    Query Ollama model using subprocess with improved response handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare the command\n",
        "        cmd = [\"ollama\", \"run\", model]\n",
        "        \n",
        "        # Start the process\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdin=subprocess.PIPE,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "            encoding='utf-8'\n",
        "        )\n",
        "        \n",
        "        # Send the prompt and get response\n",
        "        stdout, stderr = process.communicate(input=prompt, timeout=600)  # 5 minute timeout\n",
        "        \n",
        "        if process.returncode != 0:\n",
        "            return f\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ollama: {stderr}\"\n",
        "        \n",
        "        # Clean and return response\n",
        "        cleaned_response = clean_ollama_response(stdout)\n",
        "        \n",
        "        return cleaned_response\n",
        "        \n",
        "    except subprocess.TimeoutExpired:\n",
        "        return \"âŒ Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.\"\n",
        "    except FileNotFoundError:\n",
        "        return \"âŒ Ollama ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ollama ÙˆØ¥Ø¶Ø§ÙØªÙ‡ Ø¥Ù„Ù‰ PATH.\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ollama: {str(e)}\"\n",
        "\n",
        "def create_rag_prompt(question: str, search_results: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Create a concise prompt for the LLM with retrieved context\n",
        "    \"\"\"\n",
        "    \n",
        "    # Collect relevant content (take only first 2-3 most relevant results to reduce size)\n",
        "    context_texts = []\n",
        "    for i, result in enumerate(search_results[:3], 1):  # Limit to top 3 results\n",
        "        # Truncate content to reduce size\n",
        "        content = result['content'][:500] + \"...\" if len(result['content']) > 500 else result['content']\n",
        "        context_texts.append(f\"Ø§Ù„Ù…ØµØ¯Ø± {i}: {content}\")\n",
        "    \n",
        "    context = \"\\n---\\n\".join(context_texts)\n",
        "    \n",
        "    prompt = f\"\"\"Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØµØ±ÙŠ. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙÙ‚Ø·.\n",
        "\n",
        "Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n",
        "\n",
        "Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:\n",
        "{context}\n",
        "\n",
        "Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n",
        "- Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·\n",
        "- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø±ÙÙ‚Ø© ÙÙ‚Ø·\n",
        "- Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©\n",
        "- ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…Ø®ØªØµØ±Ø§Ù‹\n",
        "\n",
        "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\"\"\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "\n",
        "def enhanced_search_and_answer(query: str):\n",
        "    \"\"\"\n",
        "    Simple search function that returns question, retrieved documents, and model answer\n",
        "    \"\"\"\n",
        "    if not query.strip():\n",
        "        return {\"error\": \"ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„\"}\n",
        "    \n",
        "    # Get search results\n",
        "    results = search_documents(query, top_k=5)\n",
        "    \n",
        "    if not results:\n",
        "        return {\"error\": \"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©\"}\n",
        "    \n",
        "    # Create comprehensive prompt\n",
        "    rag_prompt = create_rag_prompt(query, results)\n",
        "    \n",
        "    # Get LLM response\n",
        "    llm_response = query_ollama(rag_prompt)\n",
        "    \n",
        "    return {\n",
        "        'question': query,\n",
        "        'retrieved_documents': results,\n",
        "        'model_answer': llm_response\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ø§Ù„Ø³Ø¤Ø§Ù„: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„Ù…ÙØ±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§ØªØŸ\n",
            "\n",
            "Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: 5\n",
            "\n",
            "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: Thinking...\n",
            "Okay, so I need to answer the question about what taxes are imposed on companies using the provided legal texts. Let me go through each source one by one.\n",
            "\n",
            "Starting with Source 1, which is in Arabic and talks about how businesses are subject to a tax rate of up to 65% on their taxable income. It mentions that this applies to partnerships and corporations taxed under the general business tax law without exempting individuals from certain gains, especially those made through ordinary activities. So, the main point here seems to be the high tax rate of 65%.\n",
            "\n",
            "Moving on to Source 2, it discusses regulations concerning shares, investments, and income tax, as well as excise duties. It mentions that companies must file reports quarterly based on their income. Additionally, there are provisions about exemptions for small investors and certain types of businesses. So, from this source, the taxes include income tax, excise duties, and quarterly reporting requirements.\n",
            "\n",
            "Source 3 is in French but seems to discuss something related to business tax again. It talks about a decrease in the excise duty on some goods due to an increase in production costs. However, it also mentions that some companies are exempt from business tax under certain conditions. So here, the key taxes mentioned are excise duties and the possibility of exemptions for specific businesses.\n",
            "\n",
            "Putting this all together, the main taxes imposed on companies seem to be:\n",
            "\n",
            "1. Business Tax at a rate up to 65% as per Source 1.\n",
            "2. Income Tax and Excise Duties as discussed in Source 2.\n",
            "3. Exemptions from business tax under certain conditions as mentioned in Source 3.\n",
            "\n",
            "I think these are the primary taxes companies in Egypt are subject to based on the provided texts. Each source adds a different aspect, so I'll need to include all of them in my answer, referencing each source appropriately.\n",
            "...done thinking.\n",
            "\n",
            "Based on the analysis of the provided legal texts, the main taxes imposed on companies in Egypt are:\n",
            "\n",
            "1. **Business Tax**: Imposed at a rate up to 65% as per Article 65 of the Law No. 24/2009. This applies to partnerships and corporations taxed under the general business tax law.\n",
            "\n",
            "2. **Income Tax and Excise Duties**: Companies are required to pay these taxes, with specific provisions for small investors and certain types of businesses (Source 2).\n",
            "\n",
            "3. **Exemptions from Business Tax**: Certain businesses may be exempt from paying business tax if they meet specific conditions outlined in the relevant legal provisions.\n",
            "\n",
            "Each of these points is supported by references to the provided sources, ensuring that the information is accurate and based on the legal texts given.\n"
          ]
        }
      ],
      "source": [
        "# ðŸš€ Test the Enhanced RAG System with Ollama\n",
        "# Make sure Ollama is running and deepseek-r1:7b model is available\n",
        "\n",
        "# Simple test - returns only question, retrieved documents, and model answer\n",
        "result = enhanced_search_and_answer(\"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¶Ø±Ø§Ø¦Ø¨ Ø§Ù„Ù…ÙØ±ÙˆØ¶Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙƒØ§ØªØŸ\")\n",
        "\n",
        "# Display the clean result\n",
        "if 'error' in result:\n",
        "    print(f\"Ø®Ø·Ø£: {result['error']}\")\n",
        "else:\n",
        "    print(f\"Ø§Ù„Ø³Ø¤Ø§Ù„: {result['question']}\")\n",
        "    print(f\"\\nØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: {len(result['retrieved_documents'])}\")\n",
        "    print(f\"\\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {result['model_answer']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "saher",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
